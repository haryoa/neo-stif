{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbee import compute_edits_and_insertions\n",
    "from insert_convert import InsertionConverter, get_number_of_masks\n",
    "from trying import PointingConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mask = 4\n",
    "use_pointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'PAD': 0, 'SWAP': 1, 'KEEP': 2, 'DELETE': 3}\n",
    "# Create Insert 1 MASK to insertion N MASKS.\n",
    "for i in range(1, max_mask+1):\n",
    "    label_map[f'KEEP|{i}'] = len(label_map)\n",
    "if not use_pointing:\n",
    "    label_map[f'DELETE|{i}'] = len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SWAP': 1,\n",
       " 'KEEP': 2,\n",
       " 'DELETE': 3,\n",
       " 'KEEP|1': 4,\n",
       " 'KEEP|2': 5,\n",
       " 'KEEP|3': 6,\n",
       " 'KEEP|4': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[KEEP]', '[KEEP]', '[DELETE]'],\n",
       " [['nasi', 'jokowi', 'goreng'], [], ['enak']])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_edits_and_insertions(['haryo', 'makan', 'jokowi'], ['haryo',  'nasi', 'jokowi', 'goreng', 'makan', 'enak'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_converter = PointingConverter({}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = point_converter.compute_points(['[CLS]', 'haryo', 'makan', '[SEP]'], ' '.join(['[CLS]', 'haryo',  'nasi', 'jokowi', 'goreng', 'makan', 'enak','[SEP]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1|, 2|nasi jokowi goreng, 3|enak, 0|]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = point_converter.compute_points(\n",
    "    [\"CLS\", \"a\", \"b\", \"a\", \"##d\", \"##e\", \"[SEP]\"],\n",
    "    \" \".join([\"[CLS]\", \"a\", \"a\", \"##d\", \"##e\", \"[SEP]\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1|, 3|, 0|, 4|, 5|, 6|, 0|]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [t.added_phrase for t in points]\n",
    "point_indexes = [t.point_index for t in points]\n",
    "point_indexes_set = set(point_indexes)\n",
    "new_labels = []\n",
    "for i, added_phrase in enumerate(labels):\n",
    "    if i not in point_indexes_set:\n",
    "        new_labels.append(label_map['DELETE'])\n",
    "    elif not added_phrase:\n",
    "        new_labels.append(label_map['KEEP'])\n",
    "    else:\n",
    "        new_labels.append(label_map['KEEP|' +\n",
    "                                            str(len(added_phrase.split()))])\n",
    "labels = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_indexes_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 3, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/haryoaw/documents/courses/nlp802/project/texteditalay/text_exiting.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/text_exiting.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m insertion_converter \u001b[39m=\u001b[39m InsertionConverter(\u001b[39m20\u001b[39m, label_map, tokenized_data, use_pointing)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
     ]
    }
   ],
   "source": [
    "insertion_converter = InsertionConverter(20, label_map, tokenized_data, use_pointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained('indolem/indobert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(max_mask=3, use_pointing=True):\n",
    "    label_map = {'PAD': 0, 'SWAP': 1, 'KEEP': 2, 'DELETE': 3}\n",
    "    # Create Insert 1 MASK to insertion N MASKS.\n",
    "    for i in range(1, max_mask+1):\n",
    "        label_map[f'KEEP|{i}'] = len(label_map)\n",
    "    if not use_pointing:\n",
    "        label_map[f'DELETE|{i}'] = len(label_map)\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = create_label_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/stif-indonesia/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# regex of xxxWORDxxx\n",
    "df_train[df_train.formal.str.contains(\"xxx.*xxx\")]\n",
    "\n",
    "# replace such words with xWORDx\n",
    "df_train.formal = df_train.formal.str.replace(\"xxx\", \"x\")\n",
    "df_train.informal = df_train.informal.str.replace(\"xxx\", \"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_instance = df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbek = compute_edits_and_insertions(example_instance.informal, example_instance.formal, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "informal_instance = tokenizer_bert.tokenize(example_instance.informal, add_special_tokens=True)\n",
    "formal_instance = tokenizer_bert.tokenize(example_instance.formal, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_converter = PointingConverter({}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = point_converter.compute_points(informal_instance, ' '.join(formal_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pointer_labels(points, label_map):\n",
    "    labels = [t.added_phrase for t in points]\n",
    "    point_indexes = [t.point_index for t in points]\n",
    "    point_indexes_set = set(point_indexes)\n",
    "    new_labels = []\n",
    "    for i, added_phrase in enumerate(labels):\n",
    "        if i not in point_indexes_set:\n",
    "            new_labels.append(label_map[\"DELETE\"])\n",
    "        elif not added_phrase:\n",
    "            new_labels.append(label_map[\"KEEP\"])\n",
    "        else:\n",
    "            new_labels.append(label_map[\"KEEP|\" + str(len(added_phrase.split()))])\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SWAP': 1,\n",
       " 'KEEP': 2,\n",
       " 'DELETE': 3,\n",
       " 'KEEP|1': 4,\n",
       " 'KEEP|2': 5,\n",
       " 'KEEP|3': 6}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = create_pointer_labels(points, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_indexes = [t.point_index for t in points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pointer_and_label(x, label_dict, point_converter):\n",
    "    informal_instance = tokenizer_bert.tokenize(x.informal, add_special_tokens=True)\n",
    "    formal_instance = tokenizer_bert.tokenize(x.formal, add_special_tokens=True)\n",
    "    points = point_converter.compute_points(informal_instance, ' '.join(formal_instance))\n",
    "    label = create_pointer_labels(points, label_dict)\n",
    "    point_indexes = [t.point_index for t in points]\n",
    "    return point_indexes, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = create_label_dict(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SWAP': 1,\n",
       " 'KEEP': 2,\n",
       " 'DELETE': 3,\n",
       " 'KEEP|1': 4,\n",
       " 'KEEP|2': 5,\n",
       " 'KEEP|3': 6,\n",
       " 'KEEP|4': 7,\n",
       " 'KEEP|5': 8,\n",
       " 'KEEP|6': 9,\n",
       " 'KEEP|7': 10,\n",
       " 'KEEP|8': 11,\n",
       " 'KEEP|9': 12,\n",
       " 'KEEP|10': 13,\n",
       " 'KEEP|11': 14,\n",
       " 'KEEP|12': 15,\n",
       " 'KEEP|13': 16,\n",
       " 'KEEP|14': 17,\n",
       " 'KEEP|15': 18,\n",
       " 'KEEP|16': 19,\n",
       " 'KEEP|17': 20,\n",
       " 'KEEP|18': 21,\n",
       " 'KEEP|19': 22,\n",
       " 'KEEP|20': 23,\n",
       " 'KEEP|21': 24,\n",
       " 'KEEP|22': 25,\n",
       " 'KEEP|23': 26,\n",
       " 'KEEP|24': 27,\n",
       " 'KEEP|25': 28,\n",
       " 'KEEP|26': 29,\n",
       " 'KEEP|27': 30,\n",
       " 'KEEP|28': 31,\n",
       " 'KEEP|29': 32,\n",
       " 'KEEP|30': 33}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_train.apply(\n",
    "    lambda x: get_pointer_and_label(x, label_dict, point_converter), axis=1\n",
    ")\n",
    "# unpack to two series\n",
    "point_indexes, label = zip(*a)\n",
    "\n",
    "# append to dataframe\n",
    "df_train['point_indexes'] = point_indexes\n",
    "df_train['label'] = label\n",
    "df_train.to_csv(\"train_with_pointing.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kalian juga tdk banyak membantu terkait dengan...</td>\n",
       "      <td>kalian juga tidak banyak membantu terkait deng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kan akun lu private , jd kaga bisa liat mereka</td>\n",
       "      <td>kan akun kamu private , jadi mereka tidak bisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min kenapa akun saya tidak ditemukan ya ? apak...</td>\n",
       "      <td>admin , mengapa akun saya tidak ditemukan ? ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akun sy sdh premium , tolong cashback yg sehar...</td>\n",
       "      <td>akun saya sudah premium , tolong cashback yang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tolong cek dm yaa min . thx !</td>\n",
       "      <td>tolong periksa dm min . terima kasih !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              informal  \\\n",
       "0    kalian juga tdk banyak membantu terkait dengan...   \n",
       "1       kan akun lu private , jd kaga bisa liat mereka   \n",
       "2    min kenapa akun saya tidak ditemukan ya ? apak...   \n",
       "3    akun sy sdh premium , tolong cashback yg sehar...   \n",
       "4                        tolong cek dm yaa min . thx !   \n",
       "..                                                 ...   \n",
       "358                                                NaN   \n",
       "359                                                NaN   \n",
       "360                                                NaN   \n",
       "361                                                NaN   \n",
       "362                                                NaN   \n",
       "\n",
       "                                                formal  \n",
       "0    kalian juga tidak banyak membantu terkait deng...  \n",
       "1    kan akun kamu private , jadi mereka tidak bisa...  \n",
       "2    admin , mengapa akun saya tidak ditemukan ? ap...  \n",
       "3    akun saya sudah premium , tolong cashback yang...  \n",
       "4               tolong periksa dm min . terima kasih !  \n",
       "..                                                 ...  \n",
       "358                                                NaN  \n",
       "359                                                NaN  \n",
       "360                                                NaN  \n",
       "361                                                NaN  \n",
       "362                                                NaN  \n",
       "\n",
       "[363 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for validation and test too\n",
    "df_val = pd.read_csv(\"../data/stif-indonesia/dev.csv\")\n",
    "df_test = pd.read_csv(\"../data/stif-indonesia/test.csv\")\n",
    "\n",
    "# replace such words with xWORDx\n",
    "df_val.formal = df_val.formal.str.replace(\"xxx\", \"x\")\n",
    "df_val.informal = df_val.informal.str.replace(\"xxx\", \"x\")\n",
    "\n",
    "# replace such words with xWORDx\n",
    "df_test.formal = df_test.formal.str.replace(\"xxx\", \"x\")\n",
    "df_test.informal = df_test.informal.str.replace(\"xxx\", \"x\")\n",
    "\n",
    "\n",
    "a = df_val.apply(\n",
    "    lambda x: get_pointer_and_label(x, label_dict, point_converter), axis=1\n",
    ")\n",
    "# unpack to two series\n",
    "point_indexes, label = zip(*a)\n",
    "df_val['point_indexes'] = point_indexes\n",
    "df_val['label'] = label\n",
    "\n",
    "df_val.to_csv(\"dev_with_pointing.csv\", index=False)\n",
    "\n",
    "a = df_test.apply(\n",
    "    lambda x: get_pointer_and_label(x, label_dict, point_converter), axis=1\n",
    ")\n",
    "# unpack to two series\n",
    "point_indexes, label = zip(*a)\n",
    "df_test['point_indexes'] = point_indexes\n",
    "df_test['label'] = label\n",
    "\n",
    "df_test.to_csv(\"test_with_pointing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.label.tolist()\n",
    "\n",
    "# flatten x\n",
    "x = [item for sublist in x for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 21447,\n",
       "         3: 12832,\n",
       "         4: 4068,\n",
       "         5: 1387,\n",
       "         6: 672,\n",
       "         7: 288,\n",
       "         8: 125,\n",
       "         9: 69,\n",
       "         10: 27,\n",
       "         11: 16,\n",
       "         12: 6,\n",
       "         14: 3,\n",
       "         13: 2,\n",
       "         17: 1,\n",
       "         15: 1,\n",
       "         16: 1})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count x\n",
    "from collections import Counter\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1922.000000\n",
       "mean       21.303330\n",
       "std         9.291961\n",
       "min         7.000000\n",
       "25%        14.000000\n",
       "50%        20.000000\n",
       "75%        27.000000\n",
       "max        87.000000\n",
       "Name: len_label, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate len describe label df_train\n",
    "df_train['len_label'] = df_train.label.apply(lambda x: len(x))\n",
    "df_train.len_label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal</th>\n",
       "      <th>formal</th>\n",
       "      <th>point_indexes</th>\n",
       "      <th>label</th>\n",
       "      <th>len_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [informal, formal, point_indexes, label, len_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.len_label > 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_with_pointing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'alhamdulillah',\n",
       " 'st',\n",
       " '##l',\n",
       " '##h',\n",
       " 'libur',\n",
       " 'x',\n",
       " '##num',\n",
       " '##ber',\n",
       " '##x',\n",
       " 'hari',\n",
       " 'on',\n",
       " '##bi',\n",
       " '##d',\n",
       " 'lg',\n",
       " '##sg',\n",
       " 'dikasih',\n",
       " 'order',\n",
       " '##an',\n",
       " ',',\n",
       " 'food',\n",
       " 'lg',\n",
       " '.',\n",
       " 'thanks',\n",
       " 'xu',\n",
       " '##ser',\n",
       " '##x',\n",
       " 'cc',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input: input_tokens, labels, point_indexes, output_tokens\n",
    "# input_tokens: tokenized input\n",
    "# labels: label (MASK|xxx, delete, keep, etc)\n",
    "# point_indexes: index of the token that is pointed to\n",
    "# output_tokens: tokenized target\n",
    "\n",
    "informal_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_source(source_tokens, labels, source_indexes,\n",
    "                        target_tokens):\n",
    "    \"\"\"Realizes source_tokens & adds deleted to source_tokens and target_tokens.\n",
    "\n",
    "    Args:\n",
    "        source_tokens: List of source tokens.\n",
    "        labels: List of label IDs, which correspond to a list of labels (KEEP,\n",
    "        DELETE, MASK|1, MASK|2...).\n",
    "        source_indexes: List of next tokens (see pointing converter for more\n",
    "        details) (ordered by source tokens)\n",
    "        target_tokens: Optional list of target tokens. Only provided when\n",
    "        constructing training examples.\n",
    "\n",
    "    Returns:\n",
    "        masked_tokens: The source input for the insertion model, including MASK\n",
    "        tokens and bracketed deleted tokens.\n",
    "        target_tokens: The target tokens for the insertion model, where mask\n",
    "        tokens are replaced with the actual token, also includes bracketed\n",
    "        deleted tokens.\n",
    "    \"\"\"\n",
    "    # well its unused...\n",
    "    DELETE_SPAN_START = '[UNK]'\n",
    "    DELETE_SPAN_END = '[PAD]'\n",
    "    current_index = 0\n",
    "    masked_tokens = []\n",
    "\n",
    "    label_map_inverse = {v: k for k, v in label_map.items()}\n",
    "\n",
    "    kept_tokens = set([0])\n",
    "    for _ in range(len(source_tokens)):\n",
    "        current_index = source_indexes[current_index]\n",
    "        kept_tokens.add(current_index)\n",
    "        # Token is deleted.\n",
    "        if current_index == 0:\n",
    "            break\n",
    "\n",
    "    current_index = 0\n",
    "    for _ in range(len(source_tokens)):\n",
    "        source_token = source_tokens[current_index]\n",
    "        deleted_tokens = []\n",
    "        # Looking forward finding all deleted tokens.\n",
    "        for i in range(current_index + 1, len(source_tokens)):\n",
    "        ## If not a deleted token.\n",
    "            if i in kept_tokens:\n",
    "                break\n",
    "            deleted_tokens.append(source_tokens[i])\n",
    "\n",
    "        # Add deleted tokens to masked_tokens and target_tokens.\n",
    "        masked_tokens.append(source_token)\n",
    "        # number_of_masks specifies the number MASKED tokens which\n",
    "        # are added to masked_tokens.\n",
    "        number_of_masks = get_number_of_masks(\n",
    "            label_map_inverse[labels[current_index]])\n",
    "        for _ in range(number_of_masks):\n",
    "            masked_tokens.append('[MASK]')\n",
    "        if deleted_tokens:\n",
    "            masked_tokens_length = len(masked_tokens)\n",
    "            bracketed_deleted_tokens = ([DELETE_SPAN_START] +\n",
    "                                    deleted_tokens +\n",
    "                                        [DELETE_SPAN_END])\n",
    "            target_tokens = (\n",
    "                target_tokens[:masked_tokens_length] + bracketed_deleted_tokens +\n",
    "                target_tokens[masked_tokens_length:])\n",
    "            masked_tokens += bracketed_deleted_tokens\n",
    "\n",
    "        current_index = source_indexes[current_index]\n",
    "        if current_index == 0:\n",
    "            break\n",
    "    return masked_tokens, target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_instance = df_train.iloc[0]\n",
    "first_label = df_train.label.tolist()[0]\n",
    "first_point_indexes = df_train.point_indexes.tolist()[0]\n",
    "informal_instance = tokenizer_bert.tokenize(example_instance.informal, add_special_tokens=True)\n",
    "formal_instance = tokenizer_bert.tokenize(example_instance.formal, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] alhamdulillah st ##l ##h libur x ##num ##ber ##x hari on ##bi ##d lg ##sg dikasih order ##an , food lg . thanks xu ##ser ##x cc [SEP]\n",
      "[CLS] alhamdulillah setelah libur x ##num ##ber ##x hari on ##bi ##d langsung diberi order , makanan lagi . terima kasih xu ##ser ##x cc . [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(informal_instance))\n",
    "print(' '.join(formal_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens, target_tokens = create_masked_source(informal_instance, \n",
    "                     first_label, \n",
    "                     first_point_indexes, \n",
    "                     formal_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_tokens and '[MASK]' not in masked_tokens:\n",
    "      # Generate random MASKs.\n",
    "    # Don't mask the start or end token.\n",
    "    indexes = list(range(1, len(masked_tokens) - 1))\n",
    "    random.shuffle(indexes)\n",
    "    # Limit MASK to ~10% of the source tokens.\n",
    "    indexes = indexes[:int(len(masked_tokens) * 0.1)]\n",
    "    for index in indexes:\n",
    "        masked_tokens[index] = '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', '[CLS]'),\n",
       " ('alhamdulillah', 'alhamdulillah'),\n",
       " ('[MASK]', 'setelah'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('st', 'st'),\n",
       " ('##l', '##l'),\n",
       " ('##h', '##h'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('libur', 'libur'),\n",
       " ('x', 'x'),\n",
       " ('##num', '##num'),\n",
       " ('##ber', '##ber'),\n",
       " ('##x', '##x'),\n",
       " ('hari', 'hari'),\n",
       " ('on', 'on'),\n",
       " ('##bi', '##bi'),\n",
       " ('##d', '##d'),\n",
       " ('[MASK]', 'langsung'),\n",
       " ('[MASK]', 'diberi'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('lg', 'lg'),\n",
       " ('##sg', '##sg'),\n",
       " ('dikasih', 'dikasih'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('order', 'order'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('##an', '##an'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " (',', ','),\n",
       " ('[MASK]', 'makanan'),\n",
       " ('[MASK]', 'lagi'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('food', 'food'),\n",
       " ('lg', 'lg'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('.', '.'),\n",
       " ('[MASK]', 'terima'),\n",
       " ('[MASK]', 'kasih'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('thanks', 'thanks'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('xu', 'xu'),\n",
       " ('##ser', '##ser'),\n",
       " ('##x', '##x'),\n",
       " ('cc', 'cc'),\n",
       " ('[MASK]', '.'),\n",
       " ('[SEP]', '[SEP]')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(masked_tokens, target_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp802",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
