{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/documents/courses/nlp802/project/texteditalay\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, BertConfig, BertForMaskedLM\n",
    "from neo_stif.components.utils import create_label_map\n",
    "import pandas as pd\n",
    "from neo_stif.components.train_data_preparation import prepare_data_tagging_and_pointer\n",
    "import datasets\n",
    "from neo_stif.lit import LitTaggerOrInsertion\n",
    "from torch.utils.data import DataLoader\n",
    "from neo_stif.components.collator import FelixCollator, FelixInsertionCollator\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping\n",
    "from neo_stif.components.utils import compute_class_weights\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "MAX_MASK = 30\n",
    "USE_POINTING = True\n",
    "\n",
    "\n",
    "model_dict = {\"koto\": \"indolem/indobert-base-uncased\"}\n",
    "\n",
    "\n",
    "LR_TAGGER = 5e-5 # due to the pre-trained nature\n",
    "LR_POINTER = 1e-5 # no pre-trained\n",
    "LR_INSERTION = 2e-5 # due to the pre-trained nature\n",
    "VAL_CHECK_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_or_name = model_dict[\"koto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 363/363 [00:00<00:00, 4748.33 examples/s]\n",
      "Map: 100%|██████████| 363/363 [00:00<00:00, 1401.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "label_dict = create_label_map(MAX_MASK, USE_POINTING)\n",
    "\n",
    "# Callback for trainer\n",
    "\n",
    "df_train = pd.read_csv(\"data/stif_indo/test_with_pointing.csv\")\n",
    "data_train = datasets.Dataset.from_pandas(df_train)\n",
    "data_train, label_dict = prepare_data_tagging_and_pointer(\n",
    "    data_train, tokenizer, label_dict\n",
    ")\n",
    "model_path_or_name = model_dict[\"koto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pre_trained_bert = BertForTokenClassification.from_pretrained(\n",
    "        model_path_or_name, num_labels=len(label_dict)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_network_config = BertConfig(\n",
    "    vocab_size=len(label_dict) + 1,\n",
    "    num_hidden_layers=2,\n",
    "    hidden_size=64,\n",
    "    num_attention_heads=1,\n",
    "    pad_token_id=len(label_dict),\n",
    ")  # + 1 as the pad token\n",
    "\n",
    "lit_tagger = LitTaggerOrInsertion.load_from_checkpoint(\n",
    "    \"/mnt/d/Documents/temp/last-v1.ckpt\",\n",
    "    model=pre_trained_bert,\n",
    "    lr=2e-5,\n",
    "    num_classes=len(label_dict),\n",
    "    class_weight=None,\n",
    "    tokenizer=tokenizer,\n",
    "    label_dict=label_dict,\n",
    "    use_pointer=USE_POINTING,\n",
    "    pointer_config=pointer_network_config,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_tagger.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_tagger = lit_tagger.eval()\n",
    "lit_tagger.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_vocab_reverse = {v: k for k, v in tokenizer.vocab.items()}\n",
    "label_dict\n",
    "\n",
    "# reverese the label dict\n",
    "label_dict_reverse = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = data_train[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '[CLS]', 'KEEP', 1, 1),\n",
      " (1, 'hal', 'KEEP', 2, 2),\n",
      " (2, 'apa', 'KEEP', 3, 3),\n",
      " (3, 'yang', 'KEEP', 4, 4),\n",
      " (4, 'lebih', 'KEEP|1', 7, 7),\n",
      " (5, 'cep', 'DELETE', 0, 0),\n",
      " (6, '##et', 'DELETE', 0, 0),\n",
      " (7, 'dari', 'KEEP', 8, 8),\n",
      " (8, 'gund', 'KEEP', 9, 9),\n",
      " (9, '##ala', 'KEEP', 10, 10),\n",
      " (10, '?', 'KEEP', 11, 11),\n",
      " (11, 'ketika', 'KEEP', 13, 13),\n",
      " (12, 'driver', 'KEEP', 0, 0),\n",
      " (13, 'dan', 'KEEP', 15, 15),\n",
      " (14, 'cs', 'KEEP', 0, 0),\n",
      " (15, 'sama', 'KEEP', 16, 16),\n",
      " (16, '-', 'KEEP', 17, 17),\n",
      " (17, 'sama', 'KEEP|4', 21, 21),\n",
      " (18, 'nge', 'DELETE', 0, 0),\n",
      " (19, '##cha', 'DELETE', 0, 0),\n",
      " (20, '##t', 'DELETE', 0, 0),\n",
      " (21, '\"', 'DELETE', 24, 24),\n",
      " (22, 'oke', 'DELETE', 0, 0),\n",
      " (23, '\"', 'DELETE', 0, 0),\n",
      " (24, '[SEP]', 'KEEP', 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    inp_to_model = tokenizer(data_0['informal'], return_tensors=\"pt\").to('cpu')\n",
    "    out_logits = lit_tagger.forward(**inp_to_model, output_hidden_states=True)\n",
    "    decoded_seq = [tokenizer_vocab_reverse[x.item()] for x in inp_to_model['input_ids'][0]]\n",
    "    decoded_label = [label_dict_reverse[x.item()] for x in out_logits.logits.argmax(-1)[0]]\n",
    "    inp_tag = torch.LongTensor([data_0['tag_labels']])\n",
    "    _, out_att = lit_tagger.forward_pointer(\n",
    "        input_ids=inp_tag,\n",
    "        attention_mask=inp_to_model[\"attention_mask\"],\n",
    "        token_type_ids=inp_to_model[\"token_type_ids\"],\n",
    "        previous_last_hidden=out_logits.hidden_states[-1],\n",
    "    )\n",
    "    att_output = out_att.argmax(-1)\n",
    "    pprint(list(zip(list(range(len(decoded_seq))), decoded_seq, decoded_label, att_output[0][0].numpy(), data_0['point_labels'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_logit, pointer_att = out_logits.logits.numpy(), out_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 34)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24, 24, 10,  5,  4, 17, 21, 17, 14, 17, 17,  5, 24,  5,  9, 24,\n",
       "         5,  5, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
       "        24, 24]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tagger_logit, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = inp_to_model['input_ids'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_index = inp_to_model['input_ids'][0].tolist().index(tokenizer.vocab['[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_tags = [\"DELETE\", \"PAD_TAG\", \"PAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = list(np.argmax(tagger_logit, axis=-1))[0]\n",
    "non_deleted_indexes = set(\n",
    "    i\n",
    "    for i, tag in enumerate(predicted_tags[: last_token_index + 1])\n",
    "    if label_dict_reverse[int(tag)] not in deleted_tags\n",
    ")\n",
    "source_tokens = [\n",
    "    tokenizer_vocab_reverse[x.item()] for x in inp_to_model[\"input_ids\"][0]\n",
    "]\n",
    "sep_indexes = set(\n",
    "    [\n",
    "        i\n",
    "        for i, token in enumerate(source_tokens)\n",
    "        if token == '[SEP]' and i in non_deleted_indexes\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 24}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_deleted_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{24}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Utility functions for running inference with a Felix model.\"\"\"\n",
    "\n",
    "from typing import Optional, Sequence, Set\n",
    "\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "def get_number_of_masks(label):\n",
    "  \"\"\"Convert a tag to the number of MASK tokens it represents.\"\"\"\n",
    "\n",
    "  if '|' not in label:\n",
    "    return 0\n",
    "  return int(label.split('|')[1])\n",
    "\n",
    "\n",
    "def _normalize_logits(logits):\n",
    "  numerator = logits\n",
    "  denominator = scipy.special.logsumexp(logits)\n",
    "  return numerator - denominator\n",
    "\n",
    "\n",
    "def beam_search_single_tagging(\n",
    "    predicted_points_logits,\n",
    "    good_indexes,\n",
    "    sep_indexes,\n",
    "    beam_size,\n",
    "    end_index = 128,\n",
    "    max_length = 128):\n",
    "  \"\"\"Returns the most likely (according to a beam search) sequence of indexes.\n",
    "\n",
    "  Args:\n",
    "    predicted_points_logits: Matrix of logits (timesteps x timesteps). Each\n",
    "      timestep has logits for every other timestep.\n",
    "    good_indexes: A restricted set of indexes which the beam must use. As such\n",
    "      the problem becomes find the most likely permutation of these indexes.\n",
    "    sep_indexes: A set of indexes for the [SEP] token. This ensure the last\n",
    "      token is a [SEP].\n",
    "    beam_size: The size of the beam.\n",
    "    end_index: The index of the last token (excluding padding)\n",
    "    max_length: The maximum length of the generation.\n",
    "\n",
    "  Returns:\n",
    "    The most likely sequence of indexes.\n",
    "  \"\"\"\n",
    "  # -1 is useful for np.argpartition which splits on smallest.\n",
    "  predicted_points = -1 * _normalize_logits(predicted_points_logits)\n",
    "  sequences = [[0]]\n",
    "  scores = [0]\n",
    "  finished_sequences = []\n",
    "  finished_scores = []\n",
    "  for _ in range(max_length):\n",
    "    assert len(sequences) == len(scores)\n",
    "    candidate_scores = []\n",
    "    candidate_sequences_reconstructor = []\n",
    "    for j, (sequence, score) in enumerate(zip(sequences, scores)):\n",
    "      sequence_set = set(sequence)\n",
    "      next_scores = predicted_points[sequence[-1]]\n",
    "      for index in range(end_index + 1):\n",
    "        # Can't predict the same index twice.\n",
    "        if index in sequence_set:\n",
    "          continue\n",
    "        # You must produce a good index.\n",
    "        if index not in good_indexes:\n",
    "          continue\n",
    "        # The last token must be a [SEP].\n",
    "        if len(sequence) == len(good_indexes) - 1:\n",
    "          if index not in sep_indexes:\n",
    "            continue\n",
    "        # If there is only one SEP don't predict it till the end.\n",
    "        elif index in sep_indexes and len(sep_indexes) == 1:\n",
    "          continue\n",
    "\n",
    "        candidate_scores.append(score + next_scores[index])\n",
    "        # Don't construct a sequence for every candidate as this is expensive.\n",
    "        # Instead store a way to reconstruct the sequence.\n",
    "        candidate_sequences_reconstructor.append((j, index))\n",
    "\n",
    "    if not candidate_scores:\n",
    "      break\n",
    "\n",
    "    if beam_size < 1:\n",
    "      break\n",
    "    if beam_size >= len(candidate_scores):\n",
    "      top_n_indexes = list(range(len(candidate_scores)))\n",
    "    else:\n",
    "      # Get the N most likely sequences. (A full sort is not needed).\n",
    "      top_n_indexes = np.argpartition(candidate_scores, beam_size)[:beam_size]\n",
    "\n",
    "    new_sequences = []\n",
    "    new_scores = []\n",
    "\n",
    "    for top_n_index in top_n_indexes:\n",
    "      sequence_index, token_index = candidate_sequences_reconstructor[\n",
    "          top_n_index]\n",
    "      # Reconstruct the sequence.\n",
    "      new_sequence = sequences[sequence_index] + [token_index]\n",
    "      new_score = candidate_scores[top_n_index]\n",
    "\n",
    "      # For every completed beam we reduce the beamsize by 1.\n",
    "      if len(new_sequence) == len(good_indexes):\n",
    "        finished_sequences.append(new_sequence)\n",
    "        finished_scores.append(-1 * new_score / len(new_sequence))\n",
    "        beam_size -= 1\n",
    "      else:\n",
    "        new_sequences.append(new_sequence)\n",
    "        new_scores.append(new_score)\n",
    "\n",
    "    sequences = new_sequences\n",
    "    scores = new_scores\n",
    "    if beam_size < 1:\n",
    "      break\n",
    "  if not finished_sequences:\n",
    "    return None\n",
    "\n",
    "  return finished_sequences[np.argmax(finished_scores)]\n",
    "\n",
    "def realize_beam_search(\n",
    "    source_token_ids,\n",
    "    ordered_source_indexes,\n",
    "    tags,\n",
    "    source_length,\n",
    "    inverse_label_map,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"Returns realized prediction using indexes and tags.\n",
    "\n",
    "    TODO: Refactor this function to share code with\n",
    "    `_create_masked_source` from insertion_converter.py to reduce code\n",
    "    duplication and to ensure that the insertion example creation is consistent\n",
    "    between preprocessing and prediction.\n",
    "\n",
    "    Args:\n",
    "      source_token_ids: List of source token ids.\n",
    "      ordered_source_indexes: The order in which the kept tokens should be\n",
    "        realized.\n",
    "      tags: a List of tags.\n",
    "      source_length: How long is the source input (excluding padding).\n",
    "\n",
    "    Returns:\n",
    "      Realized predictions (with deleted tokens).\n",
    "    \"\"\"\n",
    "    source_token_ids_set = set(ordered_source_indexes)\n",
    "    out_tokens = []\n",
    "    out_tokens_with_deletes = []\n",
    "    for j, index in enumerate(ordered_source_indexes):\n",
    "        token = tokenizer.convert_ids_to_tokens([source_token_ids[index]])\n",
    "        out_tokens += token\n",
    "        tag = inverse_label_map[tags[index]]\n",
    "        out_tokens_with_deletes += token\n",
    "        # Add the predicted MASK tokens.\n",
    "        number_of_masks = get_number_of_masks(tag)\n",
    "        # Can not add phrases after last token.\n",
    "        if j == len(ordered_source_indexes) - 1:\n",
    "            number_of_masks = 0\n",
    "        masks = [\"[MASK]\"] * number_of_masks\n",
    "        out_tokens += masks\n",
    "        out_tokens_with_deletes += masks\n",
    "\n",
    "        # Find the deleted tokens, which appear after the current token.\n",
    "        deleted_tokens = []\n",
    "        for i in range(index + 1, source_length):\n",
    "            if i in source_token_ids_set:\n",
    "                break\n",
    "            deleted_tokens.append(source_token_ids[i])\n",
    "        # Bracket the deleted tokens, between unused0 and unused1.\n",
    "        if deleted_tokens:\n",
    "            deleted_tokens = (\n",
    "                [\"[UNK]\"]\n",
    "                + list(\n",
    "                    tokenizer.convert_ids_to_tokens(deleted_tokens)\n",
    "                )\n",
    "                + [\"[PAD]\"]\n",
    "            )\n",
    "            out_tokens_with_deletes += deleted_tokens\n",
    "    return out_tokens_with_deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_np = pointer_att[0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sequence = beam_search_single_tagging(\n",
    "    list(pointer_np),\n",
    "    non_deleted_indexes,\n",
    "    sep_indexes,\n",
    "    4,\n",
    "    last_token_index,\n",
    "    20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[MASK]', '[CLS]', '[SEP]', '!']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 13, 15, 16, 17, 14, 12, 24]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "realized_inp_insertion = realize_beam_search(input_word_ids, best_sequence, predicted_tags, last_token_index+1, label_dict_reverse, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pre_trained_another_bert = BertForMaskedLM.from_pretrained(\n",
    "    model_path_or_name\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_insert = LitTaggerOrInsertion.load_from_checkpoint(\n",
    "    \"/mnt/d/Documents/temp/epoch=9-val_loss=2.21-f1_val_step=0.00.ckpt\",\n",
    "    model=pre_trained_another_bert,\n",
    "    lr=LR_INSERTION,\n",
    "    num_classes=pre_trained_another_bert.config.vocab_size,\n",
    "    class_weight=None,\n",
    "    tokenizer=tokenizer,\n",
    "    label_dict=label_dict,\n",
    "    is_insertion=True,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_insert = lit_insert.eval()\n",
    "lit_insert.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(realized_inp_insertion)\n",
    "attention_mask = [1] * len(input_ids)\n",
    "token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "# make them to torch\n",
    "input_ids = torch.LongTensor([input_ids])\n",
    "attention_mask = torch.LongTensor([attention_mask])\n",
    "token_type_ids = torch.LongTensor([token_type_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = lit_insert.forward(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "    )\n",
    "input_ids_detokenized = tokenizer.convert_ids_to_tokens(input_ids[0].numpy())\n",
    "out_ids_detokenized =  tokenizer.convert_ids_to_tokens(out.logits.argmax(-1)[0].numpy())\n",
    "\n",
    "list(zip(input_ids_detokenized, out_ids_detokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', '.'),\n",
       " ('hal', 'hal'),\n",
       " ('apa', 'apa'),\n",
       " ('yang', 'yang'),\n",
       " ('lebih', 'lebih'),\n",
       " ('[MASK]', 'cepat'),\n",
       " ('[UNK]', 'dan'),\n",
       " ('cep', 'cepat'),\n",
       " ('##et', 'cepat'),\n",
       " ('[PAD]', 'cepat'),\n",
       " ('dari', 'dari'),\n",
       " ('gund', 'gund'),\n",
       " ('##ala', '##ala'),\n",
       " ('?', '?'),\n",
       " ('ketika', 'ketika'),\n",
       " ('dan', 'dan'),\n",
       " ('sama', 'sama'),\n",
       " ('-', '-'),\n",
       " ('sama', 'sama'),\n",
       " ('[MASK]', 'ngomong'),\n",
       " ('[MASK]', '##pon'),\n",
       " ('[MASK]', 'oke'),\n",
       " ('[MASK]', 'oke'),\n",
       " ('[UNK]', ','),\n",
       " ('nge', 'nge'),\n",
       " ('##cha', '##cha'),\n",
       " ('##t', '##t'),\n",
       " ('\"', '\"'),\n",
       " ('oke', 'oke'),\n",
       " ('\"', '\"'),\n",
       " ('[PAD]', '.'),\n",
       " ('cs', 'cs'),\n",
       " ('driver', 'driver'),\n",
       " ('[SEP]', '.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizerFast' object has no attribute 'convert_ids_to_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/haryoaw/documents/courses/nlp802/project/texteditalay/notebooks/inferr_real_felix.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/notebooks/inferr_real_felix.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mconvert_ids_to_token(out\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'convert_ids_to_token'"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_token(out.logits.argmax(-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
