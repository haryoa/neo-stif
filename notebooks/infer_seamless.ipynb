{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/documents/courses/nlp802/project/texteditalay\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prepare_data' from 'neo_stif.components.train_data_preparation' (/home/haryoaw/documents/courses/nlp802/project/texteditalay/neo_stif/components/train_data_preparation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/haryoaw/documents/courses/nlp802/project/texteditalay/notebooks/infer_seamless.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/notebooks/infer_seamless.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mneo_stif\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrain_data_preparation\u001b[39;00m \u001b[39mimport\u001b[39;00m prepare_data\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/notebooks/infer_seamless.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/notebooks/infer_seamless.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'prepare_data' from 'neo_stif.components.train_data_preparation' (/home/haryoaw/documents/courses/nlp802/project/texteditalay/neo_stif/components/train_data_preparation.py)"
     ]
    }
   ],
   "source": [
    "from neo_stif.components.train_data_preparation import prepare_data\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from neo_stif.components.collator import FelixCollator\n",
    "from neo_stif.components.utils import create_label_map\n",
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1922/1922 [00:00<00:00, 13857.22 examples/s]\n",
      "Map: 100%|██████████| 1922/1922 [00:00<00:00, 2303.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, BertConfig\n",
    "\n",
    "import neo_stif\n",
    "\n",
    "MAX_MASK = 35\n",
    "USE_POINTING = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "label_dict = create_label_map(MAX_MASK, USE_POINTING)\n",
    "\n",
    "df_train = pd.read_csv(\"data/stif_indo/train_with_pointing.csv\")\n",
    "data_train = datasets.Dataset.from_pandas(df_train)\n",
    "data_train, label_dict = prepare_data(data_train, tokenizer, label_dict, max_mask=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data_train, batch_size=2, shuffle=True, collate_fn=FelixCollator(tokenizer, pad_label_as_input=len(label_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_koto = BertForTokenClassification.from_pretrained(\"indolem/indobert-base-uncased\", num_labels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo_stif.components.models import PointerNetwork\n",
    "\n",
    "\n",
    "pointer_network_config = BertConfig(\n",
    "    vocab_size=len(label_dict) + 1,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=1,\n",
    "    pad_token_id=len(label_dict),\n",
    ")  # + 1 as the pad token\n",
    "\n",
    "pointer_network = PointerNetwork(pointer_network_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_for_koto = torch.optim.AdamW(bert_koto.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_for_pointer = torch.optim.AdamW(pointer_network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TAGGER\n",
    "for current_batch in loader:\n",
    "    input_to_koto = {k: v for k, v in current_batch.items() if k in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "    \n",
    "    # Tagger\n",
    "    tag_pred = bert_koto(**input_to_koto, labels=current_batch['tag_labels'])\n",
    "    loss = tag_pred.loss\n",
    "    loss.backward()\n",
    "    optimizer_for_koto.step()\n",
    "    optimizer_for_koto.zero_grad()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1499, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TEST POINTER\n",
    "for current_batch in loader:    \n",
    "    # Pointer\n",
    "    input_to_pointer_real = {\n",
    "        k: v for k, v in current_batch.items() if k in ['tag_labels_input', 'attention_mask', 'token_type_ids']\n",
    "    }\n",
    "    input_to_pointer_real['input_ids'] = input_to_pointer_real.pop('tag_labels_input')\n",
    "    \n",
    "    point_pred = pointer_network(**input_to_pointer_real, labels=current_batch['point_labels'])\n",
    "\n",
    "    loss, att = point_pred\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimizer_for_pointer.step()\n",
    "    optimizer_for_pointer.zero_grad()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo_stif.components.extract_insertion import create_masked_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.point_indexes.iloc[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, test_processed = create_masked_source(df_train.informal, df_train.formal, df_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/stif_indo/train_with_pointing.csv\")\n",
    "data_train_insert = datasets.Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_insert['point_indexes'][5][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saya', 'makan']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"saya makan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1922/1922 [00:00<00:00, 13603.94 examples/s]\n",
      "Map: 100%|██████████| 1922/1922 [00:00<00:00, 2072.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/stif_indo/train_with_pointing.csv\")\n",
    "data_train = datasets.Dataset.from_pandas(df_train)\n",
    "data_train, label_dict = prepare_data(data_train, tokenizer, label_dict, max_mask=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['informal', 'formal', 'point_indexes', 'label', 'len_label', 'informal_input_ids', 'informal_token_type_ids', 'informal_attention_mask', 'formal_input_ids', 'formal_token_type_ids', 'formal_attention_mask', 'tag_labels', 'point_labels'],\n",
       "    num_rows: 1922\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "informal = tokenizer.tokenize(data_train['informal'][0], add_special_tokens=True)\n",
    "formal = tokenizer.tokenize(data_train['formal'][0], add_special_tokens=True)\n",
    "point_indexes = data_train['point_labels'][0]\n",
    "tag_label = data_train['tag_labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_masked_source(x, tokenizer, label_map):\n",
    "    dict_return = {}\n",
    "    informal = tokenizer.tokenize(x[\"informal\"], add_special_tokens=True)\n",
    "    formal = tokenizer.tokenize(x[\"formal\"], add_special_tokens=True)\n",
    "    point_indexes = x[\"point_labels\"]\n",
    "    tag_label = x[\"tag_labels\"]\n",
    "    masked_tokens, target_tokens = create_masked_source(\n",
    "        informal, tag_label, point_indexes, formal, label_map\n",
    "    )\n",
    "    masked_tokens_ids = [tokenizer.vocab[i] for i in masked_tokens]\n",
    "    target_tokens_ids = [tokenizer.vocab[i] for i in target_tokens]\n",
    "    attention_mask = [1] * len(masked_tokens_ids)\n",
    "    token_type_ids = [0] * len(masked_tokens_ids)\n",
    "\n",
    "    dict_return[\"input_ids\"] = torch.LongTensor(masked_tokens_ids)\n",
    "    dict_return[\"attention_mask\"] = torch.LongTensor(attention_mask)\n",
    "    dict_return[\"token_type_ids\"] = torch.LongTensor(token_type_ids)\n",
    "    dict_return[\"labels\"] = torch.LongTensor(target_tokens_ids)\n",
    "    return dict_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens, target_tokens = create_masked_source(\n",
    "    informal, tag_label, point_indexes, formal, label_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['informal', 'formal', 'point_indexes', 'label', 'len_label', 'informal_input_ids', 'informal_token_type_ids', 'informal_attention_mask', 'formal_input_ids', 'formal_token_type_ids', 'formal_attention_mask', 'tag_labels', 'point_labels'],\n",
       "    num_rows: 1922\n",
       "})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1922 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1922/1922 [18:16<00:00,  1.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_train_insertion = data_train.map(\n",
    "    process_masked_source,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"label_map\": label_dict},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1922/1922 [00:00<00:00, 89321.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save data_train_insertion parquet\n",
    "data_train_insertion.save_to_disk(\"data/stif_indo/train_insertion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] tag [UNK] nag [PAD]ih hutang [UNK] utang [PAD] ke teman saat dia terlihat sedang [UNK] temen pas doi keliatan lagi [PAD] kaya. [SEP]'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_train_insertion['labels'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MUST LOAD!\n",
    "testing = load_from_disk('data/stif_indo/train_insertion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo_stif.components.collator import FelixInsertionCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_for_insertion = BertForMaskedLM.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "insert_collator = FelixInsertionCollator(tokenizer)\n",
    "loader = DataLoader(data_train_insertion, batch_size=2, shuffle=True, collate_fn=insert_collator)\n",
    "current_batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_for_insertion = torch.optim.AdamW(pointer_network.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6082, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TEST INSERTION\n",
    "for current_batch in loader:    \n",
    "    # Pointer\n",
    "    output = bert_for_insertion(**current_batch)\n",
    "    loss = output.loss\n",
    "    loss.backward()\n",
    "    optimizer_for_insertion.step()\n",
    "    optimizer_for_insertion.zero_grad()\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    3,  1731,  1798,  9986, 10896, 18909,     2,     1,  3316,    16,\n",
       "              0,     4,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [    3,     2, 13234,     2,     2,  1975,     1,  3353,  1522,  1975,\n",
       "              0,    35,     2,     1,  5311,     0,  1684,  8168,  4143,  1959,\n",
       "           2268,     2,     4]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,    18,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100],\n",
       "         [ -100,  2811,  -100,  2447, 21273,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  1580,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,    18,  -100]])}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 11450,\n",
       " 2,\n",
       " 1,\n",
       " 1862,\n",
       " 932,\n",
       " 945,\n",
       " 0,\n",
       " 10121,\n",
       " 66,\n",
       " 8014,\n",
       " 1604,\n",
       " 962,\n",
       " 1843,\n",
       " 2587,\n",
       " 4207,\n",
       " 933,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 17849,\n",
       " 17104,\n",
       " 21463,\n",
       " 0,\n",
       " 12411,\n",
       " 1,\n",
       " 1476,\n",
       " 0,\n",
       " 16,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 14099,\n",
       " 17849,\n",
       " 0,\n",
       " 18,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 21140,\n",
       " 0,\n",
       " 18070,\n",
       " 6359,\n",
       " 962,\n",
       " 10155,\n",
       " 2,\n",
       " 4]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.vocab[i] for i in masked_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', '[CLS]'),\n",
       " ('alhamdulillah', 'alhamdulillah'),\n",
       " ('[MASK]', 'setelah'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('st', 'st'),\n",
       " ('##l', '##l'),\n",
       " ('##h', '##h'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('libur', 'libur'),\n",
       " ('x', 'x'),\n",
       " ('##num', '##num'),\n",
       " ('##ber', '##ber'),\n",
       " ('##x', '##x'),\n",
       " ('hari', 'hari'),\n",
       " ('on', 'on'),\n",
       " ('##bi', '##bi'),\n",
       " ('##d', '##d'),\n",
       " ('[MASK]', 'langsung'),\n",
       " ('[MASK]', 'diberi'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('lg', 'lg'),\n",
       " ('##sg', '##sg'),\n",
       " ('dikasih', 'dikasih'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('order', 'order'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('##an', '##an'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " (',', ','),\n",
       " ('[MASK]', 'makanan'),\n",
       " ('[MASK]', 'lagi'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('food', 'food'),\n",
       " ('lg', 'lg'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('.', '.'),\n",
       " ('[MASK]', 'terima'),\n",
       " ('[MASK]', 'kasih'),\n",
       " ('[UNK]', '[UNK]'),\n",
       " ('thanks', 'thanks'),\n",
       " ('[PAD]', '[PAD]'),\n",
       " ('xu', 'xu'),\n",
       " ('##ser', '##ser'),\n",
       " ('##x', '##x'),\n",
       " ('cc', 'cc'),\n",
       " ('[MASK]', '.'),\n",
       " ('[SEP]', '[SEP]')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(masked_tokens, target_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
