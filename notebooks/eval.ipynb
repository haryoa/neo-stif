{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/documents/courses/nlp802/project/texteditalay\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_data = [\"data/pred/lexicol_chat-gpt.txt\", \"data/pred/stif-chat-gpt.txt\"]\n",
    "test_data = [\"data/scolid/test.csv\", \"data/stif_indo/test_with_pointing.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series = []\n",
    "for file in chat_gpt_data:\n",
    "    with open(file, 'r+') as file:\n",
    "        chat_gpt_read = file.read().split('====')\n",
    "    chat_gpt_read = [line.strip() for line in chat_gpt_read]\n",
    "    chat_gpt_read = [x.split('\\n')[1] for x in chat_gpt_read if x != '']\n",
    "    chat_gpt_read = [ x.replace('<<', '').replace('>>', '') for x in chat_gpt_read]\n",
    "    df_series.append(pd.Series(chat_gpt_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_lexicol_chatgpt, series_stif_chatgpt = df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "moses_data = [\"data/pred/lexicol_moses.txt\", \"data/pred/stif-indo_moses.txt\"]\n",
    "df_series_moses = []\n",
    "for file in moses_data:\n",
    "    with open(file, 'r+') as file:\n",
    "        moses_read = file.read().split('\\n')\n",
    "    df_series_moses.append(pd.Series(moses_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_lexicol_moses, series_stif_moses = df_series_moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stif_indo_data = [\n",
    "    \"data/pred/stif-indo_felix-scratch.csv\",\n",
    "    \"data/pred/stif-indo_felix.csv\",\n",
    "    \"data/pred/stif-indo_indobart.csv\",\n",
    "    \"data/pred/stif-indo_scratch.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_series = {}\n",
    "for file in stif_indo_data:\n",
    "    # extract file name\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    stif_data = pd.read_csv(file)\n",
    "    pred_series[file_name] = pd.Series(stif_data['formal_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stif_eval = pd.DataFrame(pred_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stif_eval['stif-indo_moses'] = series_stif_moses\n",
    "df_stif_eval['stif-indo_chat-gpt'] = series_stif_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/stif_indo/test_with_pointing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stif_eval['copy'] = df_test['informal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_detokenize(text):\n",
    "    # Remove spaces before punctuation\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" !\", \"!\").replace(\" ?\", \"?\")\n",
    "    text = text.replace(\" ;\", \";\").replace(\" :\", \":\")\n",
    "\n",
    "    # Handle contractions in English (if applicable)\n",
    "    contractions = {\"n 't\": \"n't\", \" 're\": \"'re\", \" 's\": \"'s\", \" 'm\": \"'m\", \" 'll\": \"'ll\", \" 'd\": \"'d\", \" 've\": \"'ve\"}\n",
    "    for key, value in contractions.items():\n",
    "        text = text.replace(key, value)\n",
    "\n",
    "    # Additional rules can be added here for other language-specific or tokenization-specific cases\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stif-indo_felix-scratch BLEU = 1.04 45.0/5.9/0.3/0.0 (BP = 1.000 ratio = 1.413 hyp_len = 8333 ref_len = 5899)\n",
      "stif-indo_felix BLEU = 39.43 77.8/51.4/35.8/25.6 (BP = 0.901 ratio = 0.906 hyp_len = 5342 ref_len = 5899)\n",
      "stif-indo_indobart BLEU = 44.00 68.2/49.7/37.8/29.2 (BP = 1.000 ratio = 1.115 hyp_len = 6578 ref_len = 5899)\n",
      "stif-indo_scratch BLEU = 4.50 12.6/6.5/3.3/1.5 (BP = 1.000 ratio = 5.776 hyp_len = 34074 ref_len = 5899)\n",
      "stif-indo_moses BLEU = 44.90 75.4/53.0/38.9/28.7 (BP = 0.977 ratio = 0.977 hyp_len = 5763 ref_len = 5899)\n",
      "stif-indo_chat-gpt BLEU = 37.18 67.0/43.2/30.0/22.0 (BP = 1.000 ratio = 1.029 hyp_len = 6072 ref_len = 5899)\n",
      "copy BLEU = 32.51 65.3/42.0/28.8/20.4 (BP = 0.913 ratio = 0.916 hyp_len = 5406 ref_len = 5899)\n"
     ]
    }
   ],
   "source": [
    "# iterate over each column\n",
    "reference = [df_test['formal'].str.lower().apply(manual_detokenize).tolist()]\n",
    "bleu = sacrebleu.BLEU()\n",
    "\n",
    "for col in df_stif_eval.columns:\n",
    "    # check BLEU of it\n",
    "    candidate = df_stif_eval[col].str.lower().apply(str).apply(manual_detokenize).tolist()\n",
    "    print(col, bleu.corpus_score(candidate, reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bertscore = load(\"bertscore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stif-indo_felix-scratch 0.7306181847556563\n",
      "stif-indo_felix 0.8610941101040065\n",
      "stif-indo_indobart 0.8700337863165485\n",
      "stif-indo_scratch 0.6861076763838776\n",
      "stif-indo_moses 0.8804267728624265\n",
      "stif-indo_chat-gpt 0.8649380532178012\n",
      "copy 0.8283023364616163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "for col in df_stif_eval.columns:\n",
    "    # check BLEU of it\n",
    "    candidate = df_stif_eval[col].str.lower().apply(str).tolist()\n",
    "    references = reference[0]\n",
    "    results = bertscore.compute(\n",
    "        predictions=candidate, references=references, lang=\"id\"\n",
    "    )\n",
    "    print(col, np.mean(results['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['formal', 'informal', 'formal_pred'], dtype='object')\n",
      "Index(['informal', 'formal', 'formal_pred'], dtype='object')\n",
      "Index(['informal', 'formal', 'point_indexes', 'label', 'formal_predict'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# DO it for lexicol\n",
    "\n",
    "pred_series = {}\n",
    "lexicol_data = [\n",
    "    \"data/pred/lexicol_felix.csv\",\n",
    "    \"data/pred/lexicol_indobart_2.csv\",\n",
    "    \"data/pred/lexicol_scratch.csv\"\n",
    "]\n",
    "for file in lexicol_data:\n",
    "    # extract file name\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "    stif_data = pd.read_csv(file)\n",
    "    print(stif_data.columns)\n",
    "    pred_series[file_name] = pd.Series(stif_data['formal_pred' if 'formal_pred' in stif_data.columns else 'formal_predict'])\n",
    "\n",
    "df_lexicol_eval = pd.DataFrame(pred_series)\n",
    "\n",
    "df_test_lexicol = pd.read_csv(\"data/scolid/test.csv\")\n",
    "df_lexicol_eval['lexicol_moses'] = series_lexicol_moses\n",
    "df_lexicol_eval['lexicol_chat-gpt'] = series_lexicol_chatgpt\n",
    "\n",
    "df_lexicol_eval['copy'] = df_test_lexicol['informal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicol_felix BLEU = 17.64 64.4/18.7/10.1/8.0 (BP = 1.000 ratio = 1.099 hyp_len = 4575 ref_len = 4161)\n",
      "lexicol_indobart_2 BLEU = 48.88 58.3/51.8/46.0/41.1 (BP = 1.000 ratio = 1.427 hyp_len = 5939 ref_len = 4161)\n",
      "lexicol_scratch BLEU = 0.40 5.9/1.0/0.2/0.0 (BP = 1.000 ratio = 5.494 hyp_len = 22862 ref_len = 4161)\n",
      "lexicol_moses BLEU = 65.41 75.6/68.1/62.2/57.1 (BP = 1.000 ratio = 1.012 hyp_len = 4210 ref_len = 4161)\n",
      "lexicol_chat-gpt BLEU = 48.99 71.1/54.4/42.8/34.8 (BP = 1.000 ratio = 1.026 hyp_len = 4268 ref_len = 4161)\n",
      "copy BLEU = 68.53 84.4/72.6/63.7/56.6 (BP = 0.999 ratio = 0.999 hyp_len = 4158 ref_len = 4161)\n"
     ]
    }
   ],
   "source": [
    "# iterate over each column\n",
    "reference = [df_test_lexicol['formal'].str.lower().apply(manual_detokenize).tolist()]\n",
    "bleu = sacrebleu.BLEU()\n",
    "\n",
    "for col in df_lexicol_eval.columns:\n",
    "    # check BLEU of it\n",
    "    candidate = df_lexicol_eval[col].str.lower().apply(str).apply(manual_detokenize).tolist()\n",
    "    print(col, bleu.corpus_score(candidate, reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicol_felix 0.7802248302771121\n",
      "lexicol_indobart_2 0.8715773867101085\n",
      "lexicol_scratch 0.6056929048226805\n",
      "lexicol_moses 0.8961744743950513\n",
      "lexicol_chat-gpt 0.8771446303445466\n",
      "copy 0.9275806278598552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "for col in df_lexicol_eval.columns:\n",
    "    # check BLEU of it\n",
    "    candidate = df_lexicol_eval[col].str.lower().apply(str).tolist()\n",
    "    references = reference[0]\n",
    "    results = bertscore.compute(\n",
    "        predictions=candidate, references=references, lang=\"id\"\n",
    "    )\n",
    "    print(col, np.mean(results['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHeck edit distance\n",
    "\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belum', 'ada', 'konfirmasi', 'lagi', 'kah', 'min', '?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.wordpunct_tokenize(df_stif_eval['copy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each column\n",
    "reference = df_test['formal'].str.lower().apply(manual_detokenize).tolist()\n",
    "bleu = sacrebleu.BLEU()\n",
    "levenstein_ops_stif = {}\n",
    "\n",
    "for col in df_stif_eval.columns:\n",
    "    if col == 'copy':\n",
    "        continue\n",
    "    # check BLEU of it\n",
    "    candidate = df_stif_eval[col].str.lower().apply(str).apply(manual_detokenize).tolist()\n",
    "    levenstein_op_col = []\n",
    "    for i in range(len(candidate)):\n",
    "        edit_op_found = Levenshtein.opcodes(tokenize.wordpunct_tokenize(df_stif_eval['copy'][i]), tokenize.wordpunct_tokenize(candidate[i]))\n",
    "        edit_op_found = [x[0] for x in edit_op_found]\n",
    "        levenstein_op_col.append(edit_op_found)\n",
    "    levenstein_ops_stif[col] = levenstein_op_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada', 'apa', 'denganmu', '.', 'xnumberx', 'g', 'loh', 'itu']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.wordpunct_tokenize(df_stif_eval['copy'][55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada', 'apa', 'denganmu', '?', 'xnumberx', 'g', 'itu', '.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.wordpunct_tokenize(reference[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenstein_op_col = []\n",
    "\n",
    "for i in range(len(candidate)):\n",
    "    edit_op_found = Levenshtein.opcodes(tokenize.wordpunct_tokenize(df_stif_eval['copy'][i]), tokenize.wordpunct_tokenize(reference[i]))\n",
    "    edit_op_found = [x[0] for x in edit_op_found]\n",
    "    levenstein_op_col.append(edit_op_found)\n",
    "levenstein_ops_stif['gold'] = levenstein_op_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stif-indo_felix-scratch', 'stif-indo_felix', 'stif-indo_indobart', 'stif-indo_scratch', 'stif-indo_moses', 'stif-indo_chat-gpt', 'gold'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenstein_ops_stif.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stif-indo_felix-scratch\n",
      "           insert      delete     replace        keep\n",
      "count  363.000000  363.000000  363.000000  363.000000\n",
      "mean     1.867769    0.234160    2.286501    1.831956\n",
      "std      1.114511    0.449359    0.919710    1.208567\n",
      "min      0.000000    0.000000    0.000000    0.000000\n",
      "25%      1.000000    0.000000    2.000000    1.000000\n",
      "50%      2.000000    0.000000    2.000000    2.000000\n",
      "75%      2.000000    0.000000    3.000000    2.000000\n",
      "max      7.000000    2.000000    6.000000    6.000000\n",
      "insert     678\n",
      "delete      85\n",
      "replace    830\n",
      "keep       665\n",
      "dtype: int64\n",
      "stif-indo_felix\n",
      "           insert      delete     replace        keep\n",
      "count  363.000000  363.000000  363.000000  363.000000\n",
      "mean     1.033058    1.212121    2.393939    3.195592\n",
      "std      0.872545    1.046788    1.375262    1.579490\n",
      "min      0.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    0.000000    1.000000    2.000000\n",
      "50%      1.000000    1.000000    2.000000    3.000000\n",
      "75%      1.000000    2.000000    3.000000    4.000000\n",
      "max      5.000000    5.000000    8.000000    9.000000\n",
      "insert      375\n",
      "delete      440\n",
      "replace     869\n",
      "keep       1160\n",
      "dtype: int64\n",
      "stif-indo_indobart\n",
      "           insert      delete     replace        keep\n",
      "count  363.000000  363.000000  363.000000  363.000000\n",
      "mean     1.457300    0.537190    2.603306    3.099174\n",
      "std      0.860567    0.721125    1.534930    1.536328\n",
      "min      0.000000    0.000000    0.000000    1.000000\n",
      "25%      1.000000    0.000000    1.000000    2.000000\n",
      "50%      1.000000    0.000000    2.000000    3.000000\n",
      "75%      2.000000    1.000000    4.000000    4.000000\n",
      "max      4.000000    3.000000    8.000000   10.000000\n",
      "insert      529\n",
      "delete      195\n",
      "replace     945\n",
      "keep       1125\n",
      "dtype: int64\n",
      "stif-indo_scratch\n",
      "           insert      delete     replace        keep\n",
      "count  363.000000  363.000000  363.000000  363.000000\n",
      "mean     3.804408    0.228650    3.451791    4.933884\n",
      "std      1.949014    0.509637    1.763587    2.196439\n",
      "min      1.000000    0.000000    0.000000    0.000000\n",
      "25%      2.000000    0.000000    2.000000    3.000000\n",
      "50%      4.000000    0.000000    3.000000    5.000000\n",
      "75%      5.000000    0.000000    4.500000    6.000000\n",
      "max     11.000000    4.000000   10.000000   13.000000\n",
      "insert     1381\n",
      "delete       83\n",
      "replace    1253\n",
      "keep       1791\n",
      "dtype: int64\n",
      "stif-indo_moses\n",
      "           insert      delete     replace        keep\n",
      "count  363.000000  363.000000  363.000000  363.000000\n",
      "mean     1.093664    0.319559    2.677686    3.534435\n",
      "std      0.973141    0.568343    1.612528    1.635517\n",
      "min      0.000000    0.000000    0.000000    1.000000\n",
      "25%      0.000000    0.000000    2.000000    2.000000\n",
      "50%      1.000000    0.000000    2.000000    3.000000\n",
      "75%      2.000000    1.000000    3.000000    4.000000\n",
      "max      4.000000    3.000000    9.000000    9.000000\n",
      "insert      397\n",
      "delete      116\n",
      "replace     972\n",
      "keep       1283\n",
      "dtype: int64\n",
      "stif-indo_chat-gpt\n",
      "           insert      delete     replace        keep\n",
      "count  363.000000  363.000000  363.000000  363.000000\n",
      "mean     1.818182    0.432507    3.030303    3.922865\n",
      "std      1.337929    0.687490    1.650939    1.850668\n",
      "min      0.000000    0.000000    0.000000    0.000000\n",
      "25%      1.000000    0.000000    2.000000    3.000000\n",
      "50%      2.000000    0.000000    3.000000    4.000000\n",
      "75%      3.000000    1.000000    4.000000    5.000000\n",
      "max      7.000000    3.000000    9.000000   10.000000\n",
      "insert      660\n",
      "delete      157\n",
      "replace    1100\n",
      "keep       1424\n",
      "dtype: int64\n",
      "gold\n",
      "          insert      delete     replace        keep\n",
      "count  363.00000  363.000000  363.000000  363.000000\n",
      "mean     1.53719    0.432507    2.928375    3.680441\n",
      "std      1.23513    0.658762    1.592574    1.702234\n",
      "min      0.00000    0.000000    0.000000    0.000000\n",
      "25%      1.00000    0.000000    2.000000    2.000000\n",
      "50%      1.00000    0.000000    3.000000    3.000000\n",
      "75%      2.00000    1.000000    4.000000    5.000000\n",
      "max      6.00000    3.000000    8.000000   10.000000\n",
      "insert      558\n",
      "delete      157\n",
      "replace    1063\n",
      "keep       1336\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for key in levenstein_ops_stif.keys():\n",
    "    print(key)\n",
    "    op_list = levenstein_ops_stif[key]\n",
    "    op_counts = []\n",
    "    # each instance calculate number of insert, delete, replace\n",
    "    # iterate over each column\n",
    "    for i in op_list:\n",
    "        # count number of insert, delete, replace\n",
    "        insert = i.count('insert')\n",
    "        delete = i.count('delete')\n",
    "        replace = i.count('replace')\n",
    "        keep = i.count('equal')\n",
    "        op_counts.append((insert, delete, replace, keep))\n",
    "    df_op_count = pd.DataFrame(op_counts, columns=['insert', 'delete', 'replace', 'keep'])\n",
    "    print(df_op_count.describe())\n",
    "    print(df_op_count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each column\n",
    "reference = df_test_lexicol['formal'].str.lower().apply(manual_detokenize).tolist()\n",
    "bleu = sacrebleu.BLEU()\n",
    "levenstein_ops_lexicol = {}\n",
    "\n",
    "for col in df_lexicol_eval.columns:\n",
    "    if col == 'copy':\n",
    "        continue\n",
    "    # check BLEU of it\n",
    "    candidate = df_lexicol_eval[col].str.lower().apply(str).apply(manual_detokenize).tolist()\n",
    "    levenstein_op_col = []\n",
    "    for i in range(len(candidate)):\n",
    "        edit_op_found = Levenshtein.opcodes(tokenize.wordpunct_tokenize(df_lexicol_eval['copy'][i]), tokenize.wordpunct_tokenize(candidate[i]))\n",
    "        edit_op_found = [x[0] for x in edit_op_found]\n",
    "        levenstein_op_col.append(edit_op_found)\n",
    "    levenstein_ops_lexicol[col] = levenstein_op_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insert     0.791837\n",
       "delete     0.338776\n",
       "replace    3.461224\n",
       "keep       3.522449\n",
       "dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_op_count.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenstein_op_col = []\n",
    "\n",
    "for i in range(len(candidate)):\n",
    "    edit_op_found = Levenshtein.opcodes(tokenize.wordpunct_tokenize(df_lexicol_eval['copy'][i]), tokenize.wordpunct_tokenize(reference[i]))\n",
    "    edit_op_found = [x[0] for x in edit_op_found]\n",
    "    levenstein_op_col.append(edit_op_found)\n",
    "levenstein_ops_lexicol['gold'] = levenstein_op_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicol_felix\n",
      "           insert      delete    replace        keep\n",
      "count  245.000000  245.000000  245.00000  245.000000\n",
      "mean     1.069388    0.363265    2.02449    1.281633\n",
      "std      0.853682    0.764664    3.59066    3.747987\n",
      "min      0.000000    0.000000    0.00000    0.000000\n",
      "25%      1.000000    0.000000    1.00000    0.000000\n",
      "50%      1.000000    0.000000    2.00000    1.000000\n",
      "75%      1.000000    1.000000    2.00000    2.000000\n",
      "max      4.000000    9.000000   56.00000   57.000000\n",
      "insert     262\n",
      "delete      89\n",
      "replace    496\n",
      "keep       314\n",
      "dtype: int64\n",
      "lexicol_indobart_2\n",
      "           insert      delete     replace        keep\n",
      "count  245.000000  245.000000  245.000000  245.000000\n",
      "mean     0.881633    0.085714    3.277551    3.004082\n",
      "std      0.676198    0.532547    2.730465    2.786822\n",
      "min      0.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    0.000000    2.000000    1.000000\n",
      "50%      1.000000    0.000000    2.000000    2.000000\n",
      "75%      1.000000    0.000000    4.000000    4.000000\n",
      "max      4.000000    7.000000   19.000000   20.000000\n",
      "insert     216\n",
      "delete      21\n",
      "replace    803\n",
      "keep       736\n",
      "dtype: int64\n",
      "lexicol_scratch\n",
      "           insert      delete     replace        keep\n",
      "count  245.000000  245.000000  245.000000  245.000000\n",
      "mean     2.595918    0.097959    2.865306    2.367347\n",
      "std      1.532431    0.953099    1.912593    2.150999\n",
      "min      0.000000    0.000000    1.000000    0.000000\n",
      "25%      1.000000    0.000000    2.000000    1.000000\n",
      "50%      2.000000    0.000000    2.000000    2.000000\n",
      "75%      3.000000    0.000000    4.000000    3.000000\n",
      "max      7.000000   13.000000   13.000000   13.000000\n",
      "insert     636\n",
      "delete      24\n",
      "replace    702\n",
      "keep       580\n",
      "dtype: int64\n",
      "lexicol_moses\n",
      "           insert      delete     replace        keep\n",
      "count  245.000000  245.000000  245.000000  245.000000\n",
      "mean     0.379592    0.200000    2.832653    2.669388\n",
      "std      0.700404    0.755309    3.694335    3.693271\n",
      "min      0.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    0.000000    1.000000    1.000000\n",
      "50%      0.000000    0.000000    2.000000    2.000000\n",
      "75%      1.000000    0.000000    3.000000    3.000000\n",
      "max      4.000000    9.000000   38.000000   38.000000\n",
      "insert      93\n",
      "delete      49\n",
      "replace    694\n",
      "keep       654\n",
      "dtype: int64\n",
      "lexicol_chat-gpt\n",
      "           insert      delete     replace        keep\n",
      "count  245.000000  245.000000  245.000000  245.000000\n",
      "mean     0.791837    0.338776    3.461224    3.522449\n",
      "std      1.090916    0.942793    3.852230    4.093741\n",
      "min      0.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    0.000000    2.000000    2.000000\n",
      "50%      1.000000    0.000000    2.000000    2.000000\n",
      "75%      1.000000    0.000000    4.000000    4.000000\n",
      "max      7.000000    9.000000   38.000000   39.000000\n",
      "insert     194\n",
      "delete      83\n",
      "replace    848\n",
      "keep       863\n",
      "dtype: int64\n",
      "gold\n",
      "           insert      delete     replace        keep\n",
      "count  245.000000  245.000000  245.000000  245.000000\n",
      "mean     0.110204    0.077551    3.318367    3.334694\n",
      "std      0.362282    0.645052    4.554392    4.653817\n",
      "min      0.000000    0.000000    1.000000    0.000000\n",
      "25%      0.000000    0.000000    1.000000    2.000000\n",
      "50%      0.000000    0.000000    2.000000    2.000000\n",
      "75%      0.000000    0.000000    4.000000    4.000000\n",
      "max      2.000000    9.000000   56.000000   57.000000\n",
      "insert      27\n",
      "delete      19\n",
      "replace    813\n",
      "keep       817\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for key in levenstein_ops_lexicol.keys():\n",
    "    print(key)\n",
    "    op_list = levenstein_ops_lexicol[key]\n",
    "    op_counts = []\n",
    "    # each instance calculate number of insert, delete, replace\n",
    "    # iterate over each column\n",
    "    for i in op_list:\n",
    "        # count number of insert, delete, replace\n",
    "        insert = i.count('insert')\n",
    "        delete = i.count('delete')\n",
    "        replace = i.count('replace')\n",
    "        keep = i.count('equal')\n",
    "        op_counts.append((insert, delete, replace, keep))\n",
    "    df_op_count = pd.DataFrame(op_counts, columns=['insert', 'delete', 'replace', 'keep'])\n",
    "    print(df_op_count.describe())\n",
    "    print(df_op_count.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU on Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stif-indo_felix-scratch', 'stif-indo_felix', 'stif-indo_indobart', 'stif-indo_scratch', 'stif-indo_moses', 'stif-indo_chat-gpt', 'gold'])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenstein_ops_stif.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_ops_seq = list(map(lambda x: ' '.join(x), levenstein_ops_stif['gold']))\n",
    "moses_ops_seq = list(map(lambda x: ' '.join(x), levenstein_ops_stif['stif-indo_moses']))\n",
    "print(bleu.corpus_score(moses_ops_seq, [gold_ops_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stif-indo_felix-scratch BLEU = 34.40 79.0/63.1/45.5/28.1 (BP = 0.684 ratio = 0.725 hyp_len = 2258 ref_len = 3114)\n",
      "stif-indo_felix BLEU = 49.75 81.3/64.2/49.3/34.8 (BP = 0.909 ratio = 0.913 hyp_len = 2844 ref_len = 3114)\n",
      "stif-indo_indobart BLEU = 58.29 84.4/72.9/61.1/48.5 (BP = 0.892 ratio = 0.897 hyp_len = 2794 ref_len = 3114)\n",
      "stif-indo_scratch BLEU = 40.08 61.9/47.6/35.7/24.6 (BP = 1.000 ratio = 1.448 hyp_len = 4508 ref_len = 3114)\n",
      "stif-indo_moses BLEU = 57.96 85.5/74.0/60.8/48.4 (BP = 0.882 ratio = 0.889 hyp_len = 2768 ref_len = 3114)\n",
      "stif-indo_chat-gpt BLEU = 60.60 79.4/67.4/56.0/45.0 (BP = 1.000 ratio = 1.073 hyp_len = 3341 ref_len = 3114)\n"
     ]
    }
   ],
   "source": [
    "for key in levenstein_ops_stif.keys():\n",
    "    if key == 'gold':\n",
    "        continue\n",
    "    op_seq = list(map(lambda x: ' '.join(x), levenstein_ops_stif[key]))\n",
    "    print(key, bleu.corpus_score(op_seq, [gold_ops_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicol_felix BLEU = 31.76 69.4/53.2/43.3/37.5 (BP = 0.642 ratio = 0.693 hyp_len = 1161 ref_len = 1676)\n",
      "lexicol_indobart_2 BLEU = 70.44 80.1/71.9/66.0/64.8 (BP = 1.000 ratio = 1.060 hyp_len = 1776 ref_len = 1676)\n",
      "lexicol_scratch BLEU = 24.79 58.4/34.9/15.8/11.8 (BP = 1.000 ratio = 1.159 hyp_len = 1942 ref_len = 1676)\n",
      "lexicol_moses BLEU = 72.31 86.9/82.8/80.5/77.8 (BP = 0.883 ratio = 0.889 hyp_len = 1490 ref_len = 1676)\n",
      "lexicol_chat-gpt BLEU = 62.02 75.9/66.9/57.7/50.5 (BP = 1.000 ratio = 1.186 hyp_len = 1988 ref_len = 1676)\n"
     ]
    }
   ],
   "source": [
    "# Do it for lexicol too\n",
    "gold_ops_seq_lexicol = list(map(lambda x: ' '.join(x), levenstein_ops_lexicol['gold']))\n",
    "\n",
    "for key in levenstein_ops_lexicol.keys():\n",
    "    if key == 'gold':\n",
    "        continue\n",
    "    op_seq = list(map(lambda x: ' '.join(x), levenstein_ops_lexicol[key]))\n",
    "    print(key, bleu.corpus_score(op_seq, [gold_ops_seq_lexicol]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of the results\n",
    "df_stif_eval.to_csv('data/analysis/stif-indo_eval.csv')\n",
    "df_lexicol_eval.to_csv('data/analysis/lexicol_eval.csv')\n",
    "\n",
    "# levenstein_ops_stif and leexicol, save them as json\n",
    "import json\n",
    "\n",
    "with open('data/analysis/stif-indo_eval.json', 'w+') as file:\n",
    "    json.dump(levenstein_ops_stif, file)\n",
    "\n",
    "with open('data/analysis/lexicol_eval.json', 'w+') as file:\n",
    "    json.dump(levenstein_ops_lexicol, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
