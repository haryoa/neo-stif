{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/documents/courses/nlp802/project/texteditalay\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, BertConfig, BertForMaskedLM\n",
    "from neo_stif.components.utils import create_label_map\n",
    "import pandas as pd\n",
    "from neo_stif.components.train_data_preparation import prepare_data_tagging_and_pointer\n",
    "import datasets\n",
    "from neo_stif.lit import LitPointer, LitTaggerOrInsertion\n",
    "from torch.utils.data import DataLoader\n",
    "from neo_stif.components.collator import FelixCollator, FelixInsertionCollator\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint, EarlyStopping\n",
    "from neo_stif.components.utils import compute_class_weights\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "MAX_MASK = 30\n",
    "USE_POINTING = True\n",
    "\n",
    "\n",
    "model_dict = {\"koto\": \"indolem/indobert-base-uncased\"}\n",
    "\n",
    "\n",
    "LR_TAGGER = 5e-5 # due to the pre-trained nature\n",
    "LR_POINTER = 1e-5 # no pre-trained\n",
    "LR_INSERTION = 2e-5 # due to the pre-trained nature\n",
    "VAL_CHECK_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1922/1922 [00:00<00:00, 10459.69 examples/s]\n",
      "Map: 100%|██████████| 1922/1922 [00:01<00:00, 1918.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "label_dict = create_label_map(MAX_MASK, USE_POINTING)\n",
    "\n",
    "# Callback for trainer\n",
    "\n",
    "df_train = pd.read_csv(\"data/stif_indo/train_with_pointing.csv\")\n",
    "data_train = datasets.Dataset.from_pandas(df_train)\n",
    "data_train, label_dict = prepare_data_tagging_and_pointer(\n",
    "    data_train, tokenizer, label_dict\n",
    ")\n",
    "model_path_or_name = model_dict[\"koto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pre_trained_bert = BertForTokenClassification.from_pretrained(\n",
    "        model_path_or_name, num_labels=len(label_dict)\n",
    "    )\n",
    "\n",
    "pointer_network_config = BertConfig(\n",
    "        vocab_size=len(label_dict) + 1,\n",
    "        num_hidden_layers=2,\n",
    "        hidden_size=100,\n",
    "        num_attention_heads=1,\n",
    "        pad_token_id=len(label_dict),\n",
    "    )  # + 1 as the pad token\n",
    "lit_tagger = LitTaggerOrInsertion(\n",
    "    pre_trained_bert,\n",
    "    lr=LR_TAGGER,\n",
    "    num_classes=len(label_dict),\n",
    "    class_weight=None,\n",
    "    tokenizer=tokenizer,\n",
    "    label_dict=label_dict,\n",
    "    use_pointer=USE_POINTING,\n",
    "    pointer_config=pointer_network_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(data_train, batch_size=2, collate_fn=FelixCollator(tokenizer, pad_label_as_input=len(label_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_model = {\n",
    "    k: v\n",
    "    for k, v in batch.items()\n",
    "    if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "}\n",
    "\n",
    "out_tagger = lit_tagger(**input_to_model, output_hidden_states=True)\n",
    "logits, last_hidden_state = out_tagger.logits, out_tagger.hidden_states[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_model[\"input_ids\"] = batch.pop(\"tag_labels_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'labels', 'tag_labels', 'point_labels'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8405,  0.6828,  1.5589,  ...,  0.1676,  0.1404,  0.1342],\n",
      "         [ 0.0942,  0.6466,  0.9292,  ..., -0.1699,  0.4575, -0.0187],\n",
      "         [ 0.1260,  0.1223,  0.5908,  ..., -0.1454,  0.1146,  0.1259],\n",
      "         ...,\n",
      "         [ 1.4951,  0.7587,  0.4712,  ...,  0.1887,  0.5146, -0.1423],\n",
      "         [-0.1156,  0.2500,  0.4943,  ...,  0.2851,  0.2963, -0.0067],\n",
      "         [ 0.5965,  0.7373,  1.0791,  ...,  0.0721,  0.5133, -0.1444]],\n",
      "\n",
      "        [[ 0.6767,  0.5220,  0.9397,  ...,  0.4351, -0.1700, -0.0956],\n",
      "         [ 0.1796,  0.7682,  0.5418,  ...,  0.0973, -0.1072, -0.1483],\n",
      "         [-0.1553,  0.7158,  0.7125,  ..., -0.0647, -0.1159, -0.1457],\n",
      "         ...,\n",
      "         [-0.1429,  0.8854,  0.6558,  ..., -0.1628,  0.0914, -0.1264],\n",
      "         [-0.1189,  0.0296,  1.5637,  ...,  0.1906, -0.1379, -0.0803],\n",
      "         [ 0.0476,  0.4853,  1.5372,  ..., -0.0752, -0.0934, -0.1675]]],\n",
      "       grad_fn=<GeluBackward0>)\n",
      "tensor([[[ 2.5538,  1.0884,  4.3728,  ...,  0.3323, -0.0632,  0.2884],\n",
      "         [ 0.1269,  0.7058,  2.2325,  ..., -0.7858,  0.7641, -0.1549],\n",
      "         [ 0.0920, -0.4186,  1.4142,  ..., -0.4303,  0.2673,  0.1918],\n",
      "         ...,\n",
      "         [ 4.6187,  1.3035,  0.9930,  ...,  0.3296,  1.0439, -0.8865],\n",
      "         [-0.0917, -0.4342,  0.9453,  ...,  0.3510,  0.7102, -0.5372],\n",
      "         [ 1.7844,  1.1602,  3.1138,  ...,  0.0182,  1.3234, -0.7032]],\n",
      "\n",
      "        [[ 1.9183,  0.8743,  2.3283,  ...,  0.8875, -0.7376, -0.8419],\n",
      "         [ 0.4471,  1.3338,  1.1930,  ..., -0.2122, -0.9078, -0.8126],\n",
      "         [-0.4272,  1.0853,  1.4019,  ..., -0.4429, -0.6607, -0.7911],\n",
      "         ...,\n",
      "         [-0.5560,  1.8012,  1.2139,  ..., -1.0728, -0.0384, -0.9338],\n",
      "         [-0.4615, -0.7879,  4.0568,  ...,  0.1768, -0.6010, -1.0145],\n",
      "         [-0.1237,  0.5164,  4.4146,  ..., -0.7920, -0.4898, -1.3677]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(3.3687, grad_fn=<NllLossBackward0>),\n",
       " tensor([[[[0.0000, 0.0415, 0.0338,  ..., 0.0377, 0.0419, 0.0381],\n",
       "           [0.0384, 0.0408, 0.0335,  ..., 0.0424, 0.0431, 0.0386],\n",
       "           [0.0412, 0.0431, 0.0391,  ..., 0.0405, 0.0405, 0.0392],\n",
       "           ...,\n",
       "           [0.0379, 0.0448, 0.0403,  ..., 0.0354, 0.0372, 0.0343],\n",
       "           [0.0361, 0.0339, 0.0348,  ..., 0.0410, 0.0395, 0.0386],\n",
       "           [0.0397, 0.0439, 0.0350,  ..., 0.0382, 0.0409, 0.0389]]],\n",
       " \n",
       " \n",
       "         [[[0.0379, 0.0500, 0.0410,  ..., 0.0426, 0.0414, 0.0000],\n",
       "           [0.0399, 0.0450, 0.0368,  ..., 0.0445, 0.0421, 0.0000],\n",
       "           [0.0000, 0.0441, 0.0437,  ..., 0.0396, 0.0402, 0.0000],\n",
       "           ...,\n",
       "           [0.0394, 0.0451, 0.0443,  ..., 0.0459, 0.0397, 0.0000],\n",
       "           [0.0397, 0.0477, 0.0442,  ..., 0.0000, 0.0413, 0.0000],\n",
       "           [0.0400, 0.0000, 0.0485,  ..., 0.0400, 0.0420, 0.0000]]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_tagger.forward_pointer(**input_to_model, previous_last_hidden=last_hidden_state, labels=batch[\"point_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "processed_train_data = \"data/stif_indo/train_insertion\"\n",
    "processed_dev_data = \"data/stif_indo/dev_insertion\"\n",
    "batch_size=2\n",
    "device=\"cpu\"\n",
    "rich_cb = RichProgressBar()\n",
    "\n",
    "ea_stop = EarlyStopping(patience=5, monitor=\"val_loss\", mode=\"min\")\n",
    "train_data = load_from_disk(processed_train_data)\n",
    "dev_data = load_from_disk(processed_dev_data)\n",
    "train_dl = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=FelixInsertionCollator(tokenizer),\n",
    ")\n",
    "dev_dl = DataLoader(\n",
    "    dev_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=FelixInsertionCollator(tokenizer),\n",
    ")\n",
    "model = BertForMaskedLM.from_pretrained(model_path_or_name)\n",
    "lit_insert = LitTaggerOrInsertion(\n",
    "    model,\n",
    "    lr=LR_INSERTION,\n",
    "    num_classes=model.config.vocab_size,\n",
    "    class_weight=None,\n",
    "    tokenizer=tokenizer,\n",
    "    label_dict=label_dict,\n",
    "    is_insertion=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    accelerator=device,\n",
    "    devices=1,\n",
    "    val_check_interval=20,\n",
    "    check_val_every_n_epoch=None,\n",
    "    callbacks=[rich_cb, ea_stop],\n",
    "    fast_dev_run=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model   │ BertForMaskedLM  │  110 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ ce_loss │ CrossEntropyLoss │      0 │\n",
       "└───┴─────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model   │ BertForMaskedLM  │  110 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ ce_loss │ CrossEntropyLoss │      0 │\n",
       "└───┴─────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 110 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 110 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 442                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 110 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 110 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 442                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">31923\n",
       "</pre>\n"
      ],
      "text/plain": [
       "31923\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input before going to output: [('[CLS]', 'IGNORED'), ('[MASK]', 'admin'), ('[UNK]', 'IGNORED'), ('min', 'IGNORED'),\n",
       "('[PAD]', 'IGNORED'), (',', 'IGNORED'), ('promo', 'IGNORED'), ('\"', 'IGNORED'), ('makan', 'IGNORED'), (',', \n",
       "'IGNORED'), ('jajan', 'IGNORED'), ('bareng', 'IGNORED'), ('\"', 'IGNORED'), ('[MASK]', 'jika'), ('[MASK]', 'belum'),\n",
       "('[MASK]', 'premium'), ('[UNK]', 'IGNORED'), ('kalo', 'IGNORED'), ('bel', 'IGNORED'), ('##om', 'IGNORED'), \n",
       "('premi', 'IGNORED'), ('##un', 'IGNORED'), ('[PAD]', 'IGNORED'), (',', 'IGNORED'), ('cash', 'IGNORED'), ('##back', \n",
       "'IGNORED'), ('[MASK]', '##nya'), ('[MASK]', 'dapat'), ('[UNK]', 'IGNORED'), ('nya', 'IGNORED'), ('dapet', \n",
       "'IGNORED'), ('[PAD]', 'IGNORED'), ('berapa', 'IGNORED'), ('?', 'IGNORED'), ('[SEP]', 'IGNORED')]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input before going to output: [('[CLS]', 'IGNORED'), ('[MASK]', 'admin'), ('[UNK]', 'IGNORED'), ('min', 'IGNORED'),\n",
       "('[PAD]', 'IGNORED'), (',', 'IGNORED'), ('promo', 'IGNORED'), ('\"', 'IGNORED'), ('makan', 'IGNORED'), (',', \n",
       "'IGNORED'), ('jajan', 'IGNORED'), ('bareng', 'IGNORED'), ('\"', 'IGNORED'), ('[MASK]', 'jika'), ('[MASK]', 'belum'),\n",
       "('[MASK]', 'premium'), ('[UNK]', 'IGNORED'), ('kalo', 'IGNORED'), ('bel', 'IGNORED'), ('##om', 'IGNORED'), \n",
       "('premi', 'IGNORED'), ('##un', 'IGNORED'), ('[PAD]', 'IGNORED'), (',', 'IGNORED'), ('cash', 'IGNORED'), ('##back', \n",
       "'IGNORED'), ('[MASK]', '##nya'), ('[MASK]', 'dapat'), ('[UNK]', 'IGNORED'), ('nya', 'IGNORED'), ('dapet', \n",
       "'IGNORED'), ('[PAD]', 'IGNORED'), ('berapa', 'IGNORED'), ('?', 'IGNORED'), ('[SEP]', 'IGNORED')]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input, pred: [('[CLS]', ','), ('[MASK]', ','), ('[UNK]', '[UNK]'), ('min', 'min'), ('[PAD]', ')'), (',', ','), \n",
       "('promo', 'promo'), ('\"', '\"'), ('makan', 'makan'), (',', ','), ('jajan', 'jajan'), ('bareng', 'bareng'), ('\"', \n",
       "'\"'), ('[MASK]', ','), ('[MASK]', '.'), ('[MASK]', '##back'), ('[UNK]', '[UNK]'), ('kalo', 'kalo'), ('bel', 'bel'),\n",
       "('##om', '##om'), ('premi', 'premi'), ('##un', '##un'), ('[PAD]', ','), (',', ','), ('cash', 'cash'), ('##back', \n",
       "'##back'), ('[MASK]', '##nya'), ('[MASK]', ','), ('[UNK]', '[UNK]'), ('nya', 'nya'), ('dapet', 'dapet'), ('[PAD]', \n",
       "'berapa'), ('berapa', 'berapa'), ('?', '?'), ('[SEP]', '.')]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input, pred: [('[CLS]', ','), ('[MASK]', ','), ('[UNK]', '[UNK]'), ('min', 'min'), ('[PAD]', ')'), (',', ','), \n",
       "('promo', 'promo'), ('\"', '\"'), ('makan', 'makan'), (',', ','), ('jajan', 'jajan'), ('bareng', 'bareng'), ('\"', \n",
       "'\"'), ('[MASK]', ','), ('[MASK]', '.'), ('[MASK]', '##back'), ('[UNK]', '[UNK]'), ('kalo', 'kalo'), ('bel', 'bel'),\n",
       "('##om', '##om'), ('premi', 'premi'), ('##un', '##un'), ('[PAD]', ','), (',', ','), ('cash', 'cash'), ('##back', \n",
       "'##back'), ('[MASK]', '##nya'), ('[MASK]', ','), ('[UNK]', '[UNK]'), ('nya', 'nya'), ('dapet', 'dapet'), ('[PAD]', \n",
       "'berapa'), ('berapa', 'berapa'), ('?', '?'), ('[SEP]', '.')]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_insert, train_dl, dev_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
