{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbee import compute_edits_and_insertions\n",
    "from insert_convert import InsertionConverter, get_number_of_masks\n",
    "from trying import PointingConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mask = 4\n",
    "use_pointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'PAD': 0, 'SWAP': 1, 'KEEP': 2, 'DELETE': 3}\n",
    "# Create Insert 1 MASK to insertion N MASKS.\n",
    "for i in range(1, max_mask+1):\n",
    "    label_map[f'KEEP|{i}'] = len(label_map)\n",
    "if not use_pointing:\n",
    "    label_map[f'DELETE|{i}'] = len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SWAP': 1,\n",
       " 'KEEP': 2,\n",
       " 'DELETE': 3,\n",
       " 'KEEP|1': 4,\n",
       " 'KEEP|2': 5,\n",
       " 'KEEP|3': 6,\n",
       " 'KEEP|4': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[KEEP]', '[KEEP]', '[DELETE]'],\n",
       " [['nasi', 'jokowi', 'goreng'], [], ['enak']])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_edits_and_insertions(['haryo', 'makan', 'jokowi'], ['haryo',  'nasi', 'jokowi', 'goreng', 'makan', 'enak'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_converter = PointingConverter({}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = point_converter.compute_points(['[CLS]', 'haryo', 'makan', '[SEP]'], ' '.join(['[CLS]', 'haryo',  'nasi', 'jokowi', 'goreng', 'makan', 'enak','[SEP]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1|, 2|nasi jokowi goreng, 3|enak, 0|]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = point_converter.compute_points(\n",
    "    [\"CLS\", \"a\", \"b\", \"a\", \"##d\", \"##e\", \"[SEP]\"],\n",
    "    \" \".join([\"[CLS]\", \"a\", \"a\", \"##d\", \"##e\", \"[SEP]\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1|, 3|, 0|, 4|, 5|, 6|, 0|]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [t.added_phrase for t in points]\n",
    "point_indexes = [t.point_index for t in points]\n",
    "point_indexes_set = set(point_indexes)\n",
    "new_labels = []\n",
    "for i, added_phrase in enumerate(labels):\n",
    "    if i not in point_indexes_set:\n",
    "        new_labels.append(label_map['DELETE'])\n",
    "    elif not added_phrase:\n",
    "        new_labels.append(label_map['KEEP'])\n",
    "    else:\n",
    "        new_labels.append(label_map['KEEP|' +\n",
    "                                            str(len(added_phrase.split()))])\n",
    "labels = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_indexes_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 3, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/haryoaw/documents/courses/nlp802/project/texteditalay/text_exiting.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/text_exiting.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m insertion_converter \u001b[39m=\u001b[39m InsertionConverter(\u001b[39m20\u001b[39m, label_map, tokenized_data, use_pointing)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
     ]
    }
   ],
   "source": [
    "insertion_converter = InsertionConverter(20, label_map, tokenized_data, use_pointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haryoaw/mambaforge/envs/sensei/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained('indolem/indobert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(max_mask=3, use_pointing=True):\n",
    "    label_map = {'PAD': 0, 'SWAP': 1, 'KEEP': 2, 'DELETE': 3}\n",
    "    # Create Insert 1 MASK to insertion N MASKS.\n",
    "    for i in range(1, max_mask+1):\n",
    "        label_map[f'KEEP|{i}'] = len(label_map)\n",
    "    if not use_pointing:\n",
    "        label_map[f'DELETE|{i}'] = len(label_map)\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = create_label_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/stif-indonesia/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_instance = df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbek = compute_edits_and_insertions(example_instance.informal, example_instance.formal, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "informal_instance = tokenizer_bert.tokenize(example_instance.informal, add_special_tokens=True)\n",
    "formal_instance = tokenizer_bert.tokenize(example_instance.formal, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_converter = PointingConverter({}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = point_converter.compute_points(informal_instance, ' '.join(formal_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pointer_labels(points, label_map):\n",
    "    labels = [t.added_phrase for t in points]\n",
    "    point_indexes = [t.point_index for t in points]\n",
    "    point_indexes_set = set(point_indexes)\n",
    "    new_labels = []\n",
    "    for i, added_phrase in enumerate(labels):\n",
    "        if i not in point_indexes_set:\n",
    "            new_labels.append(label_map[\"DELETE\"])\n",
    "        elif not added_phrase:\n",
    "            new_labels.append(label_map[\"KEEP\"])\n",
    "        else:\n",
    "            new_labels.append(label_map[\"KEEP|\" + str(len(added_phrase.split()))])\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SWAP': 1,\n",
       " 'KEEP': 2,\n",
       " 'DELETE': 3,\n",
       " 'KEEP|1': 4,\n",
       " 'KEEP|2': 5,\n",
       " 'KEEP|3': 6}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = create_pointer_labels(points, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_indexes = [t.point_index for t in points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pointer_and_label(x, label_dict, point_converter):\n",
    "    informal_instance = tokenizer_bert.tokenize(x.informal, add_special_tokens=True)\n",
    "    formal_instance = tokenizer_bert.tokenize(x.formal, add_special_tokens=True)\n",
    "    points = point_converter.compute_points(informal_instance, ' '.join(formal_instance))\n",
    "    label = create_pointer_labels(points, label_dict)\n",
    "    point_indexes = [t.point_index for t in points]\n",
    "    return point_indexes, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = create_label_dict(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'SWAP': 1,\n",
       " 'KEEP': 2,\n",
       " 'DELETE': 3,\n",
       " 'KEEP|1': 4,\n",
       " 'KEEP|2': 5,\n",
       " 'KEEP|3': 6,\n",
       " 'KEEP|4': 7,\n",
       " 'KEEP|5': 8,\n",
       " 'KEEP|6': 9,\n",
       " 'KEEP|7': 10,\n",
       " 'KEEP|8': 11,\n",
       " 'KEEP|9': 12,\n",
       " 'KEEP|10': 13,\n",
       " 'KEEP|11': 14,\n",
       " 'KEEP|12': 15,\n",
       " 'KEEP|13': 16,\n",
       " 'KEEP|14': 17,\n",
       " 'KEEP|15': 18,\n",
       " 'KEEP|16': 19,\n",
       " 'KEEP|17': 20,\n",
       " 'KEEP|18': 21,\n",
       " 'KEEP|19': 22,\n",
       " 'KEEP|20': 23,\n",
       " 'KEEP|21': 24,\n",
       " 'KEEP|22': 25,\n",
       " 'KEEP|23': 26,\n",
       " 'KEEP|24': 27,\n",
       " 'KEEP|25': 28,\n",
       " 'KEEP|26': 29,\n",
       " 'KEEP|27': 30,\n",
       " 'KEEP|28': 31,\n",
       " 'KEEP|29': 32,\n",
       " 'KEEP|30': 33}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_train.apply(\n",
    "    lambda x: get_pointer_and_label(x, label_dict, point_converter), axis=1\n",
    ")\n",
    "# unpack to two series\n",
    "point_indexes, label = zip(*a)\n",
    "\n",
    "# append to dataframe\n",
    "df_train['point_indexes'] = point_indexes\n",
    "df_train['label'] = label\n",
    "df_train.to_csv(\"train_with_pointing.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for validation and test too\n",
    "df_val = pd.read_csv(\"../data/stif-indonesia/dev.csv\")\n",
    "df_test = pd.read_csv(\"../data/stif-indonesia/test.csv\")\n",
    "\n",
    "a = df_val.apply(\n",
    "    lambda x: get_pointer_and_label(x, label_dict, point_converter), axis=1\n",
    ")\n",
    "# unpack to two series\n",
    "point_indexes, label = zip(*a)\n",
    "df_val['point_indexes'] = point_indexes\n",
    "df_val['label'] = label\n",
    "\n",
    "df_val.to_csv(\"dev_with_pointing.csv\", index=False)\n",
    "\n",
    "a = df_test.apply(\n",
    "    lambda x: get_pointer_and_label(x, label_dict, point_converter), axis=1\n",
    ")\n",
    "# unpack to two series\n",
    "point_indexes, label = zip(*a)\n",
    "df_test['point_indexes'] = point_indexes\n",
    "df_test['label'] = label\n",
    "\n",
    "df_test.to_csv(\"test_with_pointing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.label.tolist()\n",
    "\n",
    "# flatten x\n",
    "x = [item for sublist in x for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 22550,\n",
       "         3: 12954,\n",
       "         4: 4069,\n",
       "         5: 1386,\n",
       "         6: 670,\n",
       "         7: 290,\n",
       "         8: 124,\n",
       "         9: 68,\n",
       "         10: 29,\n",
       "         11: 15,\n",
       "         12: 5,\n",
       "         13: 4,\n",
       "         14: 3,\n",
       "         17: 1,\n",
       "         15: 1,\n",
       "         16: 1})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count x\n",
    "from collections import Counter\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1922.000000\n",
       "mean       21.940687\n",
       "std        10.131238\n",
       "min         7.000000\n",
       "25%        14.000000\n",
       "50%        20.000000\n",
       "75%        28.000000\n",
       "max       100.000000\n",
       "Name: len_label, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate len describe label df_train\n",
    "df_train['len_label'] = df_train.label.apply(lambda x: len(x))\n",
    "df_train.len_label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal</th>\n",
       "      <th>formal</th>\n",
       "      <th>point_indexes</th>\n",
       "      <th>label</th>\n",
       "      <th>len_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>pagi admin , paket saya dari shopee dengan kode xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx r04b tidak kebaca nomor resinya , mohon dibantu ya . dari hari jumat masih ngendep di jne malang</td>\n",
       "      <td>pagi admin . paket saya dari shopee dengan kode xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx r04b tidak terbaca nomor resinya . mohon dibantu . dari hari jumat masih mengendap di jne malang .</td>\n",
       "      <td>[1, 2, 80, 0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 0, 0, 74, 75, 77, 0, 78, 81, 0, 4, 82, 83, 84, 88, 0, 0, 0, 89, 90, 91, 92, 0]</td>\n",
       "      <td>[2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 2, 2, 4, 3, 2, 4, 3, 2, 2, 2, 2, 4, 3, 3, 3, 2, 2, 2, 4, 2]</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>sudah , harga barang xxxnumberxxx xxxnumberxxx xxxnumberxxx k , ongkir xxxnumberxxx xxxnumberxxx k , diskon xxxnumberxxx xxxnumberxxx k , biaya penanganan xxxnumberxxx , xxxnumberxxx k . seharusnya saya bayar xxxnumberxxx , xxxnumberxxx k saja , tapi malah ditagih xxxnumberxxx , xxxnumberxxx k .</td>\n",
       "      <td>sudah , harga barang xxxnumberxxx ribu , onkir xxxnumberxxx ribu , diskon xxxnumberxxx ribu , biaya penanganan xxxnumberxxx ribu . seharusnya saya bayar xxxnumberxxx ribu saja , tapi malah ditagih xxxnumberxxx ribu .</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 24, 25, 26, 27, 28, 35, 0, 0, 0, 0, 0, 0, 36, 37, 38, 39, 40, 41, 48, 0, 0, 0, 0, 0, 0, 49, 50, 51, 52, 53, 54, 55, 63, 0, 0, 0, 0, 0, 0, 0, 64, 65, 66, 67, 68, 69, 70, 71, 79, 0, 0, 0, 0, 0, 0, 0, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 98, 0, 0, 0, 0, 0, 0, 0, 99, 0]</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2]</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                      informal  \\\n",
       "780   pagi admin , paket saya dari shopee dengan kode xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx r04b tidak kebaca nomor resinya , mohon dibantu ya . dari hari jumat masih ngendep di jne malang            \n",
       "1220  sudah , harga barang xxxnumberxxx xxxnumberxxx xxxnumberxxx k , ongkir xxxnumberxxx xxxnumberxxx k , diskon xxxnumberxxx xxxnumberxxx k , biaya penanganan xxxnumberxxx , xxxnumberxxx k . seharusnya saya bayar xxxnumberxxx , xxxnumberxxx k saja , tapi malah ditagih xxxnumberxxx , xxxnumberxxx k .   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                 formal  \\\n",
       "780   pagi admin . paket saya dari shopee dengan kode xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx xxxnumberxxx r04b tidak terbaca nomor resinya . mohon dibantu . dari hari jumat masih mengendap di jne malang .   \n",
       "1220  sudah , harga barang xxxnumberxxx ribu , onkir xxxnumberxxx ribu , diskon xxxnumberxxx ribu , biaya penanganan xxxnumberxxx ribu . seharusnya saya bayar xxxnumberxxx ribu saja , tapi malah ditagih xxxnumberxxx ribu .                                                                            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                            point_indexes  \\\n",
       "780   [1, 2, 80, 0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 0, 0, 74, 75, 77, 0, 78, 81, 0, 4, 82, 83, 84, 88, 0, 0, 0, 89, 90, 91, 92, 0]   \n",
       "1220  [1, 2, 3, 4, 5, 6, 7, 8, 9, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 24, 25, 26, 27, 28, 35, 0, 0, 0, 0, 0, 0, 36, 37, 38, 39, 40, 41, 48, 0, 0, 0, 0, 0, 0, 49, 50, 51, 52, 53, 54, 55, 63, 0, 0, 0, 0, 0, 0, 0, 64, 65, 66, 67, 68, 69, 70, 71, 79, 0, 0, 0, 0, 0, 0, 0, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 98, 0, 0, 0, 0, 0, 0, 0, 99, 0]             \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                             label  \\\n",
       "780   [2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 2, 2, 4, 3, 2, 4, 3, 2, 2, 2, 2, 4, 3, 3, 3, 2, 2, 2, 4, 2]                        \n",
       "1220  [2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2]   \n",
       "\n",
       "      len_label  \n",
       "780   93         \n",
       "1220  100        "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.len_label > 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_with_pointing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'alhamdulillah',\n",
       " 'st',\n",
       " '##l',\n",
       " '##h',\n",
       " 'libur',\n",
       " 'xxx',\n",
       " '##num',\n",
       " '##ber',\n",
       " '##xx',\n",
       " '##x',\n",
       " 'hari',\n",
       " 'on',\n",
       " '##bi',\n",
       " '##d',\n",
       " 'lg',\n",
       " '##sg',\n",
       " 'dikasih',\n",
       " 'order',\n",
       " '##an',\n",
       " ',',\n",
       " 'food',\n",
       " 'lg',\n",
       " '.',\n",
       " 'thanks',\n",
       " 'xxx',\n",
       " '##user',\n",
       " '##xx',\n",
       " '##x',\n",
       " 'cc',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input: input_tokens, labels, point_indexes, output_tokens\n",
    "# input_tokens: tokenized input\n",
    "# labels: label (MASK|xxx, delete, keep, etc)\n",
    "# point_indexes: index of the token that is pointed to\n",
    "# output_tokens: tokenized target\n",
    "\n",
    "informal_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[UNUSED]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/haryoaw/documents/courses/nlp802/project/texteditalay/text_exiting.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/text_exiting.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer_bert\u001b[39m.\u001b[39;49mvocab[\u001b[39m'\u001b[39;49m\u001b[39m[UNUSED]\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: '[UNUSED]'"
     ]
    }
   ],
   "source": [
    "tokenizer_bert.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_source(source_tokens, labels, source_indexes,\n",
    "                        target_tokens):\n",
    "    \"\"\"Realizes source_tokens & adds deleted to source_tokens and target_tokens.\n",
    "\n",
    "    Args:\n",
    "        source_tokens: List of source tokens.\n",
    "        labels: List of label IDs, which correspond to a list of labels (KEEP,\n",
    "        DELETE, MASK|1, MASK|2...).\n",
    "        source_indexes: List of next tokens (see pointing converter for more\n",
    "        details) (ordered by source tokens)\n",
    "        target_tokens: Optional list of target tokens. Only provided when\n",
    "        constructing training examples.\n",
    "\n",
    "    Returns:\n",
    "        masked_tokens: The source input for the insertion model, including MASK\n",
    "        tokens and bracketed deleted tokens.\n",
    "        target_tokens: The target tokens for the insertion model, where mask\n",
    "        tokens are replaced with the actual token, also includes bracketed\n",
    "        deleted tokens.\n",
    "    \"\"\"\n",
    "    DELETE_SPAN_START = '[UNK]'\n",
    "    DELETE_SPAN_END = '[PAD]'\n",
    "    current_index = 0\n",
    "    masked_tokens = []\n",
    "\n",
    "    label_map_inverse = {v: k for k, v in label_map.items()}\n",
    "\n",
    "    kept_tokens = set([0])\n",
    "    for _ in range(len(source_tokens)):\n",
    "        current_index = source_indexes[current_index]\n",
    "        kept_tokens.add(current_index)\n",
    "        # Token is deleted.\n",
    "        if current_index == 0:\n",
    "            break\n",
    "\n",
    "    current_index = 0\n",
    "    for _ in range(len(source_tokens)):\n",
    "        source_token = source_tokens[current_index]\n",
    "        deleted_tokens = []\n",
    "        # Looking forward finding all deleted tokens.\n",
    "        for i in range(current_index + 1, len(source_tokens)):\n",
    "        ## If not a deleted token.\n",
    "            if i in kept_tokens:\n",
    "                break\n",
    "            deleted_tokens.append(source_tokens[i])\n",
    "\n",
    "        # Add deleted tokens to masked_tokens and target_tokens.\n",
    "        masked_tokens.append(source_token)\n",
    "        # number_of_masks specifies the number MASKED tokens which\n",
    "        # are added to masked_tokens.\n",
    "        number_of_masks = get_number_of_masks(\n",
    "            label_map_inverse[labels[current_index]])\n",
    "        for _ in range(number_of_masks):\n",
    "            masked_tokens.append('[MASK]')\n",
    "        if deleted_tokens:\n",
    "            masked_tokens_length = len(masked_tokens)\n",
    "            bracketed_deleted_tokens = ([DELETE_SPAN_START] +\n",
    "                                    deleted_tokens +\n",
    "                                        [DELETE_SPAN_END])\n",
    "            target_tokens = (\n",
    "                target_tokens[:masked_tokens_length] + bracketed_deleted_tokens +\n",
    "                target_tokens[masked_tokens_length:])\n",
    "            masked_tokens += bracketed_deleted_tokens\n",
    "\n",
    "        current_index = source_indexes[current_index]\n",
    "        if current_index == 0:\n",
    "            break\n",
    "    return masked_tokens, target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_instance = df_train.iloc[0]\n",
    "first_label = df_train.label.tolist()[0]\n",
    "first_point_indexes = df_train.point_indexes.tolist()[0]\n",
    "informal_instance = tokenizer_bert.tokenize(example_instance.informal, add_special_tokens=True)\n",
    "formal_instance = tokenizer_bert.tokenize(example_instance.formal, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] alhamdulillah st ##l ##h libur xxx ##num ##ber ##xx ##x hari on ##bi ##d lg ##sg dikasih order ##an , food lg . thanks xxx ##user ##xx ##x cc [SEP]\n",
      "[CLS] alhamdulillah setelah libur xxx ##num ##ber ##xx ##x hari on ##bi ##d langsung diberi order , makanan lagi . terima kasih xxx ##user ##xx ##x cc . [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(informal_instance))\n",
    "print(' '.join(formal_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens, target_tokens = create_masked_source(informal_instance, \n",
    "                     first_label, \n",
    "                     first_point_indexes, \n",
    "                     formal_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_tokens and '[MASK]' not in masked_tokens:\n",
    "      # Generate random MASKs.\n",
    "    # Don't mask the start or end token.\n",
    "    indexes = list(range(1, len(masked_tokens) - 1))\n",
    "    random.shuffle(indexes)\n",
    "    # Limit MASK to ~10% of the source tokens.\n",
    "    indexes = indexes[:int(len(masked_tokens) * 0.1)]\n",
    "    for index in indexes:\n",
    "        masked_tokens[index] = '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', '[CLS]'),\n",
       " ('alhamdulillah', 'alhamdulillah'),\n",
       " ('[MASK]', 'setelah'),\n",
       " ('[unused1]', '[unused1]'),\n",
       " ('st', 'st'),\n",
       " ('##l', '##l'),\n",
       " ('##h', '##h'),\n",
       " ('[unused2]', '[unused2]'),\n",
       " ('libur', 'libur'),\n",
       " ('xxx', 'xxx'),\n",
       " ('##num', '##num'),\n",
       " ('##ber', '##ber'),\n",
       " ('##xx', '##xx'),\n",
       " ('##x', '##x'),\n",
       " ('hari', 'hari'),\n",
       " ('on', 'on'),\n",
       " ('##bi', '##bi'),\n",
       " ('##d', '##d'),\n",
       " ('[MASK]', 'langsung'),\n",
       " ('[MASK]', 'diberi'),\n",
       " ('[unused1]', '[unused1]'),\n",
       " ('lg', 'lg'),\n",
       " ('##sg', '##sg'),\n",
       " ('dikasih', 'dikasih'),\n",
       " ('[unused2]', '[unused2]'),\n",
       " ('order', 'order'),\n",
       " ('[unused1]', '[unused1]'),\n",
       " ('##an', '##an'),\n",
       " ('[unused2]', '[unused2]'),\n",
       " (',', ','),\n",
       " ('[MASK]', 'makanan'),\n",
       " ('[MASK]', 'lagi'),\n",
       " ('[unused1]', '[unused1]'),\n",
       " ('food', 'food'),\n",
       " ('lg', 'lg'),\n",
       " ('[unused2]', '[unused2]'),\n",
       " ('.', '.'),\n",
       " ('[MASK]', 'terima'),\n",
       " ('[MASK]', 'kasih'),\n",
       " ('[unused1]', '[unused1]'),\n",
       " ('thanks', 'thanks'),\n",
       " ('[unused2]', '[unused2]'),\n",
       " ('xxx', 'xxx'),\n",
       " ('##user', '##user'),\n",
       " ('##xx', '##xx'),\n",
       " ('##x', '##x'),\n",
       " ('cc', 'cc'),\n",
       " ('[MASK]', '.'),\n",
       " ('[SEP]', '[SEP]')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(masked_tokens, target_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp802",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
