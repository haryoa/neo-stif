{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo create pointing network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertLayer, BertConfig, BertAttention\n",
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_config = BertConfig(num_attention_heads=1, hidden_size=768, num_hidden_layers=2, intermediate_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointer network\n",
    "tag_embedding = torch.nn.Embedding(12,5)\n",
    "pos_embedding = torch.nn.Embedding(100,5)\n",
    "\n",
    "# Linear SWISH GELU\n",
    "linear = torch.nn.Linear(5,5)\n",
    "swish = torch.nn.SiLU()\n",
    "gelu = torch.nn.GELU()\n",
    "\n",
    "# 2x encoder\n",
    "encoder = BertLayer(pointer_config)\n",
    "\n",
    "# Attention Layer! (single head)\n",
    "attention = BertAttention(pointer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train_with_pointing.csv\")\n",
    "data_train = datasets.Dataset.from_pandas(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(max_mask=3, use_pointing=True):\n",
    "    label_map = {'PAD': 0, 'SWAP': 1, 'KEEP': 2, 'DELETE': 3}\n",
    "    # Create Insert 1 MASK to insertion N MASKS.\n",
    "    for i in range(1, max_mask+1):\n",
    "        label_map[f'KEEP|{i}'] = len(label_map)\n",
    "    if not use_pointing:\n",
    "        label_map[f'DELETE|{i}'] = len(label_map)\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = create_label_dict(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbee import compute_edits_and_insertions\n",
    "from insert_convert import InsertionConverter, get_number_of_masks\n",
    "from trying import PointingConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_converter = PointingConverter({}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_src_tgt(examples, tokenizer, src=\"informal\", tgt=\"formal\"):\n",
    "    returned_dict = {f\"{src}_{i}\": j for i,j in tokenizer(examples[src]).items()}\n",
    "    returned_dict.update({f\"{tgt}_{i}\": j for i,j in tokenizer(examples[tgt]).items()})\n",
    "    return returned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1922/1922 [00:00<00:00, 9236.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.map(\n",
    "    tokenize_function_src_tgt,\n",
    "    batched=True,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['informal', 'formal', 'point_indexes', 'label', 'informal_input_ids', 'informal_token_type_ids', 'informal_attention_mask', 'formal_input_ids', 'formal_token_type_ids', 'formal_attention_mask'],\n",
       "    num_rows: 1922\n",
       "})"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pointer_labels(points, label_map):\n",
    "    labels = [t.added_phrase for t in points]\n",
    "    point_indexes = [t.point_index for t in points]\n",
    "    point_indexes_set = set(point_indexes)\n",
    "    new_labels = []\n",
    "    for i, added_phrase in enumerate(labels):\n",
    "        if i not in point_indexes_set:\n",
    "            new_labels.append(label_map[\"DELETE\"])\n",
    "        elif not added_phrase:\n",
    "            new_labels.append(label_map[\"KEEP\"])\n",
    "        else:\n",
    "            new_labels.append(label_map[\"KEEP|\" + str(len(added_phrase.split()))])\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenized(examples, tokenizer, label_dict, point_converter, src=\"informal\", tgt=\"formal\"):\n",
    "    src_tokenized = tokenizer.tokenize(examples[src], add_special_tokens=True)\n",
    "    tgt_tokenized = tokenizer.tokenize(examples[tgt], add_special_tokens=True)\n",
    "    points = point_converter.compute_points(src_tokenized, ' '.join(tgt_tokenized))\n",
    "    label = create_pointer_labels(points, label_dict)\n",
    "    point_indexes = [t.point_index for t in points] \n",
    "    # change them to torch tensors\n",
    "    label = label\n",
    "    point_indexes = point_indexes\n",
    "    return {f\"tag_labels\": label, f\"point_labels\": point_indexes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1922/1922 [00:02<00:00, 929.97 examples/s] \n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.map(\n",
    "    generate_tokenized,\n",
    "    batched=False,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label_dict\": label_dict,\n",
    "        \"point_converter\": point_converter\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['informal', 'formal', 'point_indexes', 'label', 'informal_input_ids', 'informal_token_type_ids', 'informal_attention_mask', 'formal_input_ids', 'formal_token_type_ids', 'formal_attention_mask', 'tag_labels', 'point_labels'],\n",
       "    num_rows: 1922\n",
       "})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['point_labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['formal_input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['informal', 'formal', 'point_indexes', 'label', 'informal_input_ids', 'informal_token_type_ids', 'informal_attention_mask', 'formal_input_ids', 'formal_token_type_ids', 'formal_attention_mask', 'tag_labels', 'point_labels'],\n",
       "    num_rows: 1922\n",
       "})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_koto = BertForTokenClassification.from_pretrained(\"indolem/indobert-base-uncased\", num_labels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator\n",
    "\n",
    "\n",
    "class FelixCollator:\n",
    "    def __init__(self, tokenizer, pad_label=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_label = pad_label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch is a list of dicts\n",
    "        output_dict = {}\n",
    "        informal_input_ids, informal_attention_mask = [\n",
    "            [i[col] for i in batch]\n",
    "            for col in [\n",
    "                \"informal_input_ids\",\n",
    "                \"informal_attention_mask\",\n",
    "            ]\n",
    "        ]\n",
    "        formal_input_ids = [i[\"formal_input_ids\"] for i in batch]\n",
    "\n",
    "        tag_label = [i[\"tag_labels\"] for i in batch]\n",
    "\n",
    "        tokenized_output = self.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": informal_input_ids,\n",
    "                \"attention_mask\": informal_attention_mask,\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokenized_output[\"token_type_ids\"] = torch.zeros_like(\n",
    "            tokenized_output[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "        output_label = self.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": formal_input_ids,\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        out_label = output_label[\"input_ids\"]\n",
    "        # change pad token to -100\n",
    "        out_label[out_label == self.tokenizer.pad_token_id] = self.pad_label\n",
    "        print(out_label)\n",
    "        output_dict.update(tokenized_output)\n",
    "        output_dict['labels'] = out_label\n",
    "        print(output_dict)\n",
    "        # add tag_label to output_dict\n",
    "        # each tag_label is a list of labels (list) with different length\n",
    "        # pad first\n",
    "        max_len = max([len(i) for i in tag_label])\n",
    "        tag_label = [i + [self.pad_label] * (max_len - len(i)) for i in tag_label]\n",
    "        tag_label = torch.tensor(tag_label)\n",
    "\n",
    "        output_dict[\"tag_labels\"] = tag_label\n",
    "\n",
    "        # add point_label to output_dict\n",
    "        # same as above\n",
    "        point_label = [i[\"point_labels\"] for i in batch]\n",
    "        max_len = max([len(i) for i in point_label])\n",
    "        point_label = [i + [self.pad_label] * (max_len - len(i)) for i in point_label]\n",
    "        point_label = torch.tensor(point_label)\n",
    "\n",
    "        output_dict[\"point_labels\"] = point_label\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(data_train, batch_size=2, collate_fn=FelixCollator(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    3, 11450,  1818, 10121, 21474,  8014,  1604, 10518,   962,  1843,\n",
      "          2587,  4207,   933,  2460,  3716, 12411,    16,  3005,  1975,    18,\n",
      "          5218,  3774, 21474, 27330, 10518,   962, 10155,    18,     4],\n",
      "        [    3,  4863,  6097,  4374,    18,  1731,  2882,  3888,  6505,  1881,\n",
      "         10896,  4362,  1925,  2643,  6813,    16,     6,  2022,  4942,  1560,\n",
      "          2289,     6,  9153, 23120,    18,  5218,  3774,    18,     4]])\n",
      "{'input_ids': tensor([[    3, 11450,  1862,   932,   945, 10121, 21474,  8014,  1604, 10518,\n",
      "           962,  1843,  2587,  4207,   933, 17849, 17104, 21463, 12411,  1476,\n",
      "            16, 14099, 17849,    18, 21140, 21474, 27330, 10518,   962, 10155,\n",
      "             4],\n",
      "        [    3,  4863,  6097,  2118,    18,  1731,  2882,  3888,  6505,  1881,\n",
      "         10896,  4362,    16,  1925,  2643,  6813,     6,  2022,  4942,  1560,\n",
      "          2289,     6,  9153, 23120,    18,  5218,  3774,     4,     0,     0,\n",
      "             0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[    3, 11450,  1818, 10121, 21474,  8014,  1604, 10518,   962,  1843,\n",
      "          2587,  4207,   933,  2460,  3716, 12411,    16,  3005,  1975,    18,\n",
      "          5218,  3774, 21474, 27330, 10518,   962, 10155,    18,     4],\n",
      "        [    3,  4863,  6097,  4374,    18,  1731,  2882,  3888,  6505,  1881,\n",
      "         10896,  4362,  1925,  2643,  6813,    16,     6,  2022,  4942,  1560,\n",
      "          2289,     6,  9153, 23120,    18,  5218,  3774,    18,     4]])}\n",
      "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'labels', 'tag_labels', 'point_labels'])\n",
      "torch.Size([2, 29])\n",
      "torch.Size([2, 31]) token_type_ids\n",
      "torch.Size([2, 31]) input_ids\n",
      "torch.Size([2, 31]) attention_mask\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for current_batch in loader:\n",
    "    print(current_batch.keys())\n",
    "    input_to_koto = {k: v for k, v in current_batch.items() if k in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "    print(current_batch['labels'].shape)\n",
    "    for x in ['token_type_ids', 'input_ids', 'attention_mask']:\n",
    "        print(input_to_koto[x].shape, x)\n",
    "    tag_pred = bert_koto(**input_to_koto, labels=current_batch['tag_labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (29) must match the size of tensor b (31) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/haryoaw/documents/courses/nlp802/project/texteditalay/testing_inference_backprop.ipynb Cell 25\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/testing_inference_backprop.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m input_to_koto \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m current_batch\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/testing_inference_backprop.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m input_to_koto[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m current_batch[\u001b[39m'\u001b[39m\u001b[39mtag_label\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/haryoaw/documents/courses/nlp802/project/texteditalay/testing_inference_backprop.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m bert_koto(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_to_koto)\n",
      "File \u001b[0;32m~/mambaforge/envs/sensei/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/sensei/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1756\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1756\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1757\u001b[0m     input_ids,\n\u001b[1;32m   1758\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1759\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1760\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1761\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1762\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1763\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1764\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1765\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1766\u001b[0m )\n\u001b[1;32m   1768\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/mambaforge/envs/sensei/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/sensei/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1018\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1019\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/sensei/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/sensei/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:235\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    232\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m    233\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m--> 235\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39;49m token_type_embeddings\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (29) must match the size of tensor b (31) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "current_batch = next(iter(loader))\n",
    "\n",
    "input_to_koto = {k: v for k, v in current_batch.items() if k in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "input_to_koto['labels'] = current_batch['tag_label']\n",
    "bert_koto(**input_to_koto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    3, 11450,  1818, 10121, 21474,  8014,  1604, 10518,   962,  1843,\n",
       "           2587,  4207,   933,  2460,  3716, 12411,    16,  3005,  1975,    18,\n",
       "           5218,  3774, 21474, 27330, 10518,   962, 10155,    18,     4],\n",
       "         [    3,  4863,  6097,  4374,    18,  1731,  2882,  3888,  6505,  1881,\n",
       "          10896,  4362,  1925,  2643,  6813,    16,     6,  2022,  4942,  1560,\n",
       "           2289,     6,  9153, 23120,    18,  5218,  3774,    18,     4]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[    3, 11450,  1818, 10121, 21474,  8014,  1604, 10518,   962,  1843,\n",
       "           2587,  4207,   933,  2460,  3716, 12411,    16,  3005,  1975,    18,\n",
       "           5218,  3774, 21474, 27330, 10518,   962, 10155,    18,     4],\n",
       "         [    3,  4863,  6097,  4374,    18,  1731,  2882,  3888,  6505,  1881,\n",
       "          10896,  4362,  1925,  2643,  6813,    16,     6,  2022,  4942,  1560,\n",
       "           2289,     6,  9153, 23120,    18,  5218,  3774,    18,     4]]),\n",
       " 'tag_label': tensor([[   2,    4,    3,    3,    3,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    5,    3,    3,    3,    2,    3,    5,    3,    3,    5,\n",
       "             3,    2,    2,    2,    2,    4,    2],\n",
       "         [   2,    2,    4,    3,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    4,    2, -100, -100, -100]]),\n",
       " 'point_labels': tensor([[   1,    5,    0,    0,    0,    6,    7,    8,    9,   10,   11,   12,\n",
       "            13,   14,   18,    0,    0,    0,   20,    0,   23,    0,    0,   25,\n",
       "             0,   26,   27,   28,   29,   30,    0],\n",
       "         [   1,    2,    4,    0,    5,    6,    7,    8,    9,   10,   11,   13,\n",
       "            16,   14,   15,   12,   17,   18,   19,   20,   21,   22,   23,   24,\n",
       "            25,   26,   27,    0, -100, -100, -100]])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POINTER NETWORK INFERENCE\n",
    "\n",
    "def _realize_beam_search(self, source_token_ids,\n",
    "                           ordered_source_indexes,\n",
    "                           tags,\n",
    "                           source_length):\n",
    "    \"\"\"Returns realized prediction using indexes and tags.\n",
    "\n",
    "    TODO: Refactor this function to share code with\n",
    "    `_create_masked_source` from insertion_converter.py to reduce code\n",
    "    duplication and to ensure that the insertion example creation is consistent\n",
    "    between preprocessing and prediction.\n",
    "\n",
    "    Args:\n",
    "      source_token_ids: List of source token ids.\n",
    "      ordered_source_indexes: The order in which the kept tokens should be\n",
    "        realized.\n",
    "      tags: a List of tags.\n",
    "      source_length: How long is the source input (excluding padding).\n",
    "\n",
    "    Returns:\n",
    "      Realized predictions (with deleted tokens).\n",
    "    \"\"\"\n",
    "    # Need to help type checker.\n",
    "    self._inverse_label_map = cast(Mapping[int, str], self._inverse_label_map)\n",
    "\n",
    "    source_token_ids_set = set(ordered_source_indexes)\n",
    "    out_tokens = []\n",
    "    out_tokens_with_deletes = []\n",
    "    for j, index in enumerate(ordered_source_indexes):\n",
    "      token = self._builder.tokenizer.convert_ids_to_tokens(\n",
    "          [source_token_ids[index]])\n",
    "      out_tokens += token\n",
    "      tag = self._inverse_label_map[tags[index]]\n",
    "      if self._use_open_vocab:\n",
    "        out_tokens_with_deletes += token\n",
    "        # Add the predicted MASK tokens.\n",
    "        number_of_masks = insertion_converter.get_number_of_masks(tag)\n",
    "        # Can not add phrases after last token.\n",
    "        if j == len(ordered_source_indexes) - 1:\n",
    "          number_of_masks = 0\n",
    "        masks = [constants.MASK] * number_of_masks\n",
    "        out_tokens += masks\n",
    "        out_tokens_with_deletes += masks\n",
    "\n",
    "        # Find the deleted tokens, which appear after the current token.\n",
    "        deleted_tokens = []\n",
    "        for i in range(index + 1, source_length):\n",
    "          if i in source_token_ids_set:\n",
    "            break\n",
    "          deleted_tokens.append(source_token_ids[i])\n",
    "        # Bracket the deleted tokens, between unused0 and unused1.\n",
    "        if deleted_tokens:\n",
    "          deleted_tokens = [constants.DELETE_SPAN_START] + list(\n",
    "              self._builder.tokenizer.convert_ids_to_tokens(deleted_tokens)) + [\n",
    "                  constants.DELETE_SPAN_END\n",
    "              ]\n",
    "          out_tokens_with_deletes += deleted_tokens\n",
    "      # Add the predicted phrase.\n",
    "      elif '|' in tag:\n",
    "        pos_pipe = tag.index('|')\n",
    "        added_phrase = tag[pos_pipe + 1:]\n",
    "        out_tokens.append(added_phrase)\n",
    "\n",
    "    if not self._use_open_vocab:\n",
    "      out_tokens_with_deletes = out_tokens\n",
    "    assert (\n",
    "        out_tokens_with_deletes[0] == (constants.CLS)\n",
    "    ), (f' {out_tokens_with_deletes} did not start/end with the correct tokens '\n",
    "        f'{constants.CLS}, {constants.SEP}')\n",
    "    return out_tokens_with_deletes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_embedding = self._tag_embedding_layer(edit_tags)\n",
    "position_embedding = self._position_embedding_layer(tag_embedding)\n",
    "edit_tagged_sequence_output = self._edit_tagged_sequence_output_layer(\n",
    "    tf.keras.layers.concatenate(\n",
    "        [bert_output, tag_embedding, position_embedding]))\n",
    "\n",
    "intermediate_query_embeddings = edit_tagged_sequence_output\n",
    "if self._bert_config.query_transformer:\n",
    "    attention_mask = self._self_attention_mask_layer(\n",
    "        intermediate_query_embeddings, input_mask)\n",
    "    for _ in range(int(self._bert_config.query_transformer)):\n",
    "    intermediate_query_embeddings = self._transformer_query_layer(\n",
    "        [intermediate_query_embeddings, attention_mask])\n",
    "\n",
    "query_embeddings = self._query_embeddings_layer(\n",
    "    intermediate_query_embeddings)\n",
    "\n",
    "key_embeddings = self._key_embeddings_layer(edit_tagged_sequence_output)\n",
    "\n",
    "pointing_logits = self._attention_scores(query_embeddings, key_embeddings,\n",
    "                                            tf.cast(input_mask, tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
